{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook de test de gensim pour le projet fil rouge\n",
    "Exploration de la librairie GENSIM.\n",
    "L'idée est de générer les mots clés sur quelques fichiers arXive pour lesquels j'ai déjà les mots-clés."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim as gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy# Plotting tools\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    nlp =  spacy.cli.download(\"en_core_web_sm\") # la première fois, on passe ici\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use']) # on peut l'étendre"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gensim  utilise logging (contrairement à owlready2), je l'initialise pour voir ce qui se passe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "J'utilise pandas pour lire un fichier csv de données.\n",
    "Il s'agit d'un tableau contenant pour 10 articles arXive le titre, l'abstract et les mots clés"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"testGENSIM.csv\",delimiter=';',header=0,encoding='Windows-1252') # encoding d'excel sous windows..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df.shape\n",
    "NbTexte , _ = df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "           Entry_id                                              title  \\\n0  arXiv:1503.04220  Fuzzy Mixed Integer Optimization Model for Reg...   \n1  arXiv:1511.02420  Design of an Alarm System for Isfahan Ozone Le...   \n2  arXiv:1603.09728  A Survey of League Championship Algorithm: Pro...   \n3  arXiv:1509.00690  A Fuzzy Approach for Feature Evaluation and Di...   \n4  arXiv:1501.05940  A New Efficient Method for Calculating Similar...   \n\n                                             summary  \\\n0  Mixed Integer Optimization has been a topic of...   \n1  The ozone level prediction is an important tas...   \n2  The League Championship Algorithm (LCA) is spo...   \n3  Web Usage Mining is the application of data mi...   \n4  Web services allow communication between heter...   \n\n                                         KeyWORDS(c)  \\\n0  Mixed Integer Optimization, Fuzzy Sets, Regres...   \n1       ozone predictor, artificial intelligence, UV   \n2   Global­Optimization,­ League­Championship­ Al...   \n3   web usage mining; fuzzy c-means clustering, f...   \n4   web service ; semantic similarity ; syntactic...   \n\n                                            KeyWORDS  \n0  –Mixed Integer Optimization; Fuzzy Sets; Regre...  \n1  – ozone predictor, artificial intelligence, UV...  \n2   ­Global­Optimization,­ League­Championship­ A...  \n3   web usage mining; fuzzy c-means clustering, f...  \n4   web service ; semantic similarity ; syntactic...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entry_id</th>\n      <th>title</th>\n      <th>summary</th>\n      <th>KeyWORDS(c)</th>\n      <th>KeyWORDS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>arXiv:1503.04220</td>\n      <td>Fuzzy Mixed Integer Optimization Model for Reg...</td>\n      <td>Mixed Integer Optimization has been a topic of...</td>\n      <td>Mixed Integer Optimization, Fuzzy Sets, Regres...</td>\n      <td>–Mixed Integer Optimization; Fuzzy Sets; Regre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>arXiv:1511.02420</td>\n      <td>Design of an Alarm System for Isfahan Ozone Le...</td>\n      <td>The ozone level prediction is an important tas...</td>\n      <td>ozone predictor, artificial intelligence, UV</td>\n      <td>– ozone predictor, artificial intelligence, UV...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arXiv:1603.09728</td>\n      <td>A Survey of League Championship Algorithm: Pro...</td>\n      <td>The League Championship Algorithm (LCA) is spo...</td>\n      <td>Global­Optimization,­ League­Championship­ Al...</td>\n      <td>­Global­Optimization,­ League­Championship­ A...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>arXiv:1509.00690</td>\n      <td>A Fuzzy Approach for Feature Evaluation and Di...</td>\n      <td>Web Usage Mining is the application of data mi...</td>\n      <td>web usage mining; fuzzy c-means clustering, f...</td>\n      <td>web usage mining; fuzzy c-means clustering, f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>arXiv:1501.05940</td>\n      <td>A New Efficient Method for Calculating Similar...</td>\n      <td>Web services allow communication between heter...</td>\n      <td>web service ; semantic similarity ; syntactic...</td>\n      <td>web service ; semantic similarity ; syntactic...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va faire un premier nettoyage du texte en retirant les parentheses.\n",
    "On peut utiliser https://regex101.com/ pour tester les regex"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import re\n",
    "# Define a function to map the values\n",
    "def correct_text(text):\n",
    "    text1 = text.replace('(', '')\n",
    "    text2 = text1.replace(')','')\n",
    "    text3 = re.sub(\"\\d*-\\d*-\\d*\", '', text2)  # on retire les dates\n",
    "    text4 = re.sub(\"\\d{4}\",'',text3) # on retire les années\n",
    "    text5 = re.sub(\"(\\d+\\.{1}\\d+)\",'',text4)  # on retire les valeurs numériques\n",
    "    return text5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df['cleared_summary'] = df['summary'].apply(correct_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "data = df.cleared_summary.values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = df.summary.values.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'The League Championship Algorithm LCA is sport-inspired optimization algorithm that was introduced by Ali Husseinzadeh Kashan in the year . It has since drawn enormous interest among the researchers because of its potential efficiency in solving many optimization problems and real-world applications. The LCA has also shown great potentials in solving non-deterministic polynomial time NP-complete problems. This survey presents a brief synopsis of the LCA literatures in peer-reviewed journals, conferences and book chapters. These research articles are then categorized according to indexing in the major academic databases Web of Science, Scopus, IEEE Xplore and the Google Scholar. The analysis was also done to explore the prospects and the challenges of the algorithm and its acceptability among researchers. This systematic categorization can be used as a basis for future studies.'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construction du corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#text = \"\"\n",
    "#for i in range(NbTexte):\n",
    "#    text = text + \" \" + data.summary[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "todo retirer les années, les dates, les parentheses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import re\n",
    "# Remove Emails\n",
    "#data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "# Remove new line characters\n",
    "#data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "#data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "#print(data[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize(split) the sentences into words\n",
    "lmots = [[text for text in phrase.split(\" \")] for phrase in data[0].split(\".\")]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(lmots)\n",
    "print(dictionary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(lmots)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict  # For word frequency\n",
    "word_freq = defaultdict(int)\n",
    "for sent in data:\n",
    "    print(\"sent:\",sent)\n",
    "    mots = sent.split(\" \")\n",
    "    for i in mots:#sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "phrases = gensim.models.Phrases(data, min_count=30, progress_per=10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(phrases)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenize words and cleanup the text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):  # from https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Bigram and Trigram models\n",
    "\n",
    "Bigrams are 2 words frequently occuring together in docuent. Trigrams are 3 words frequently occuring. Many other techniques are explained in part-1 of the blog which are important in NLP pipline, it would be worth your while going through that blog. The 2 arguments for Phrases are min_count and threshold. The higher the values of these parameters , the harder its for a word to be combined to bigram.\n",
    "https://radimrehurek.com/gensim/models/phrases.html\n",
    "indique Automatically detect common phrases – aka multi-word expressions, word n-gram collocations – from a stream of sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10)#100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)#0)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define function for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\")#, disable=['parser', 'ner']) # au depart seulement 'en'\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "print(data_words_nostops)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data_words_bigrams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dictionary and Corpus needed for Topic Modeling\n",
    "Make sure to check if dictionary[id2word] or corpus is clean otherwise you may not get good quality topics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Gensim creates unique id for each word in the document. Its mapping of word_id and word_frequency. Example: (8,2) above indicates, word_id 8 occurs twice in the document and so on.\n",
    "    This is used as input to LDA model.\n",
    "\n",
    "If you want to see what word corresponds to a given id, then pass the id as a key to dictionary. Example: id2word[4].\n",
    "\n",
    "    Readable format of corpus can be obtained by executing below code block."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building topic model\n",
    "\n",
    "Parameters of LDA\n",
    "\n",
    "    Alpha and Beta are Hyperparameters — alpha represents document-topic density and Beta represents topic-word density, chunksize is the number of documents to be used in each training chunk, update_every determines how often the model parameters should be updated and passes is the total number of training passes.\n",
    "    A measure for best number of topics really depends on kind of corpus you are using, the size of corpus, number of topics you expect to see."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=NbTexte/2,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "View topics in LDA model\n",
    "\n",
    "    Each topic is combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "    You can see keywords for each topic and weightage of each keyword using lda_model.print_topics()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the keyword of topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lda_model.show_topic(4)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    topk = lda_model.show_topic(topic_id, 10)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "\n",
    "    print('{}: {}'.format(topic_id, ' '.join(topk_words)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les topics 1 et 2 sont tres proches. J'ai produit trop de topic -> je vais en faire que 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Predicting the topics for a document\n",
    "\n",
    "If you have a new document, you can use the trained model to estimate the topic proportions for it.\n",
    "\n",
    "This is done in two steps: first, the document is converted into a matrix, and then the inference is carried out.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc = data[0].split()\n",
    "\n",
    "doc_vector = lda_model.id2word.doc2bow(doc)\n",
    "doc_topics = lda_model[doc_vector]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "The result shows predicted topic distribution. In most cases, there will be one or more dominant topics, and small probabilities for the rest of the topics.\n",
    "\n",
    "For instance, for the document this book describes Windows software, we will typically get a result that this document is a mix of a book-related topic and a software-related topic. (Compare to the topic list you got above.) Again, the exact result here will vary between executions because of issues related to random number generation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_topics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for phrase, score in gensim.models.Phrases.find_phrases(doc).items():\n",
    "\n",
    "    print(phrase, score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
