{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exploration du dépot arXiv et de l'exploitation de PDF\n",
    "\n",
    "Ce notebook est une exploration générale du projet fil rouge.\n",
    "Il s'agit ici de \"découvrir\" arXiv et l'exploitation directe d'un pdf.\n",
    "\n",
    "arXiv (https://github.com/lukasschwab/arxiv.py) est une librairie d'interogation du dépôt arXiv qui respecte nativement les contraintes de restriction d'appel. (Encore qu'avec google colaboratory, on peut utilement depasser un banissement par arXiv.) \n",
    "\n",
    "Installons la :"
   ],
   "metadata": {
    "id": "9-QkadHgMQC1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRV5LP-wMVWk",
    "outputId": "7e2ca7c2-567d-48a5-a466-421cde1f6492"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting arXiv\n",
      "  Downloading arxiv-1.4.3-py3-none-any.whl (12 kB)\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.1/81.1 KB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=fdcb3e6f0e7c4510b6c399a2463163a759011e51b165762edc2fec7d79ac1390\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/7a/a7/78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arXiv\n",
      "Successfully installed arXiv-1.4.3 feedparser-6.0.10 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test sur un petit fichier plutot ancien \n",
    "Commencons par un petit et ancien article."
   ],
   "metadata": {
    "id": "9PQCdqHmz8rh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCOXslgiMaJ3",
    "outputId": "7476b974-12f5-4220-d2a8-3f57cf370d2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://arxiv.org/abs/1512.05986v1\n",
      "2015-12-18 15:16:31+00:00\n",
      "2015-12-18 15:16:31+00:00\n",
      "Can Pretrained Neural Networks Detect Anatomy?\n",
      "[arxiv.Result.Author('Vlado Menkovski'), arxiv.Result.Author('Zharko Aleksovski'), arxiv.Result.Author('Axel Saalbach'), arxiv.Result.Author('Hannes Nickisch')]\n",
      "Convolutional neural networks demonstrated outstanding empirical results in\n",
      "computer vision and speech recognition tasks where labeled training data is\n",
      "abundant. In medical imaging, there is a huge variety of possible imaging\n",
      "modalities and contrasts, where annotated data is usually very scarce. We\n",
      "present two approaches to deal with this challenge. A network pretrained in a\n",
      "different domain with abundant data is used as a feature extractor, while a\n",
      "subsequent classifier is trained on a small target dataset; and a deep\n",
      "architecture trained with heavy augmentation and equipped with sophisticated\n",
      "regularization methods. We test the approaches on a corpus of X-ray images to\n",
      "design an anatomy detection system.\n",
      "NIPS 2015 Workshop on Machine Learning in Healthcare\n",
      "None\n",
      "None\n",
      "cs.CV\n",
      "['cs.CV', 'cs.AI', 'cs.NE']\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/1512.05986v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1512.05986v1', title='pdf', rel='related', content_type=None)]\n",
      "http://arxiv.org/pdf/1512.05986v1\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query = \"1512.05986\"  # pour avoir un petit fichier !\n",
    ")\n",
    "\n",
    "# Les informations disponibles via le site arXiv :\n",
    "for result in search.results():\n",
    "    print(result.entry_id) # A url http://arxiv.org/abs/{id}.\n",
    "    print(result.updated) # When the result was last updated.\n",
    "    print(result.published) # When the result was originally published.\n",
    "    print(result.title) # The title of the result.\n",
    "    print(result.authors) # The result's authors, as arxiv.Authors.\n",
    "    print(result.summary) # The result abstract.\n",
    "    print(result.comment) # The authors' comment if present.\n",
    "    print(result.journal_ref) # A journal reference if present.\n",
    "    print(result.doi) # A URL for the resolved DOI to an external resource if present.\n",
    "    print(result.primary_category) # The result's primary arXiv category. See arXiv: Category Taxonomy.\n",
    "    print(result.categories) # All of the result's categories. See arXiv: Category Taxonomy.\n",
    "    print(result.links) # Up to three URLs associated with this result, as arxiv.Links.\n",
    "    print(result.pdf_url) # "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remarquons que si les fichiers disposent d'un identifiant, les auteurs n'en n'ont pas.\n",
    "\n",
    "Si on va sur le lien de téléchargement, on constate que les auteurs sont données via des liens hypertextes. Par exemple sur le premier : https://arxiv.org/search/cs?searchtype=author&query=Menkovski%2C+V.\n",
    "Il s'agit d'une recherche sur le nom, parmi les auteurs du domaine computer science. La personne qui soumet l'article, peut fournir son identifiant d'auteur ORCID, mais il n'est pas exposé actuellement. On aura donc des problèmes d'homonymie.\n",
    "\n",
    "\n",
    "Téléchargeons ce fichier en local."
   ],
   "metadata": {
    "id": "zD16YE4ONRgi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "filename = './'+result.pdf_url[21:]+'.pdf'\n",
    "\n",
    "# Envoi d'une requête pour récupérer le contenu du PDF\n",
    "response = requests.get(result.pdf_url)\n",
    "\n",
    "# Vérification du statut de la réponse\n",
    "if response.status_code == 200:\n",
    "    # Récupération du contenu du PDF et enregistrement dans un fichier local   \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        print('PDF téléchargé avec succès !')\n",
    "else:\n",
    "    print('Impossible de télécharger le PDF. Code d\\'erreur:', response.status_code)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgTKte38M2Xe",
    "outputId": "b9953a4c-7daa-49ef-f87e-daa4ee32fda1"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PDF téléchargé avec succès !\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Passons par une lecture directe du pdf\n",
    "\n",
    "Nous utiliserons pyPDF2 qui propose quelques fonctionnalités interessantes :\n",
    "* une extraction en fonction de l'orientation du texte (qui permet par exemple de retirer le filigrane ou au contraire de ne récuperer que lui [cela permet de ne pas utiliser le nom du fichier, mais les PDF les plus anciens n'ont pas de filigrane]) ;\n",
    "* on peut egalement introduire un filtrage en fonction de la position du caractère sur la page. (Mais cette fonctionalité ne s'est pas avérée concluante pour filtrer les en-tête et les pieds de page car chaque PDF aura ses propres paramètres.)"
   ],
   "metadata": {
    "id": "ZKKt2t-_0Q55"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "16BQC3owK-tU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93ac722d-2d79-4fa9-9ed0-751d0fc88f08"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m232.6/232.6 KB\u001B[0m \u001B[31m17.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from pypdf2) (4.5.0)\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "page_content = \"\"\n",
    "with open(filename, \"rb\") as pdf_file:\n",
    "    read_pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    number_of_pages = len(read_pdf.pages)\n",
    "    for i in range(number_of_pages):\n",
    "      page = read_pdf.pages[i]\n",
    "      # on recupère seulement le texte horizontal (supprime le filigrane)\n",
    "      page_content = page_content +'\\n' + page.extract_text(orientations=0)\n",
    "\n",
    "print(page_content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSIS9yLXsBwB",
    "outputId": "336fb2fb-9823-49c2-8c92-4ad8bde94dc8"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "CanPretrainedNeuralNetworksDetectAnatomy?\n",
      "VladoMenkovski\n",
      "PhilipsResearch\n",
      "Eindhoven,Netherlands\n",
      "vlado.menkovski@philips.comZharkoAleksovski\n",
      "PhilipsResearch\n",
      "Eindhoven,Netherlands\n",
      "zharko.aleksovski@philips.com\n",
      "Axel Saalbach\n",
      "PhilipsResearch\n",
      "Hamburg,Germany\n",
      "axel.saalbach@philips.coomHannesNickisch\n",
      "PhilipsResearch\n",
      "Hamburg,Germany\n",
      "hannes.nickisch@philips.com\n",
      "Abstract\n",
      "Convolutional neural networks demonstrated outstanding e mpirical results in\n",
      "computervisionandspeechrecognitiontaskswherelabeled trainingdataisabun-\n",
      "dant. In medical imaging, there is a huge variety of possible imaging modalities\n",
      "and contrasts, where annotated data is usually very scarce. We present two ap-\n",
      "proaches to deal with this challenge. A network pretrained i n a different domain\n",
      "with abundant data is used as a feature extractor, while a sub sequent classiﬁer is\n",
      "trained on a small target dataset; and a deep architecture tr ained with heavy aug-\n",
      "mentation and equipped with sophisticated regularization methods. We test the\n",
      "approachesonacorpusofX-rayimagestodesignananatomyde tectionsystem.\n",
      "1 Introduction and Motivation\n",
      "Deep learning [9] (DL) is rapidly gaining momentum in comput er vision [5], speech recognition\n",
      "[3] and medical imaging applications due to their record-br eakingempirical performanceand their\n",
      "promiseto jointlylearnthelow-levelfeaturesandthehigh -leveldecisionsdirectlyfromrawdata.\n",
      "One major open research challenge is the transfer of the exce llent predictive performance of DL\n",
      "methods,inparticularconvolutionalneuralnetworks(CNN ), fromthe“data-laden”regimeofcom-\n",
      "putervision1to the“data-bounded”regimeofmedicalimaging.\n",
      "Since the typical number of parametersin the DL models is ver y big the major concernfor operat-\n",
      "ing in “data-bounded”regimesis inability to generalizean d form robust featuresbecausethe small\n",
      "number of examples do not capture well the variability in the input space. There are various ap-\n",
      "proaches to deal with this challenge. Data augmentation can be used to increase the effective size\n",
      "of the training set and drive the training to build invarianc es to speciﬁc transformations. Network\n",
      "architecturedesigncanbeusedtohard-codepriorknowledg e(suchastranslationinvariance)byde-\n",
      "sign. Sophisticatedregularizationmethodssuchasdropou t[8]andbatchnormalization(BN)[4]can\n",
      "worktocounteroverﬁtting. Finally, globaltrainingofthefeatureextractionlayersonabundantdata\n",
      "domains (i.e. natural images) and then training the “classi ﬁcation” higher layers from the limited\n",
      "datadomainscanbeusedtoensurethatthelower-levelfeatu resareinvarianttotheparticularitiesof\n",
      "thetrainingdataset.\n",
      "Wechoosetwodirectionstoinvestigatetheseapproachesfo rthegoalofanatomydetectioninX-ray\n",
      "images. The ﬁrst using a network trained on a large corpusof n atural images for feature extraction\n",
      "1There are 106images in 103classes in the ILSVRC challenge,\n",
      "http://www.image-net.org/challenges/LSVRC .\n",
      "1\n",
      "combined with a SVM [1] classiﬁer trained on the medical imag es. And a second approach using\n",
      "heavy augmentation and regularization of a deep neural netw ork trained only on the small-sized\n",
      "medicalimagedataset.\n",
      "2 Experiments andResults\n",
      "In order to assess the performance of speciﬁcally trained an d off-the-shelf networks, radiographs\n",
      "from the ImageClef 2009 - Medical Image Annotation task2were used. This challenge database\n",
      "consists of a broadrange of X-rayimagesfromclinical routi ne,with a detailed anatomicalclassiﬁ-\n",
      "cation. For convenience, we ﬂattened the hierarchical clas s representation and disregarded classes\n",
      "withfewerthan50exampleimages. Indoingsoweendedupwith 24uniqueclasses. Forevaluation\n",
      "purposes,theentiredatacorpus(of14676images)wasdivid edintoa trainingandtest set, covering\n",
      "90%and10%ofthe datarespectively.\n",
      "As a ﬁrst baseline, the OverFeat network (see [6]) in combina tion with a (linear) multi-class SVM\n",
      "was employed. OverFeat is a convolutional neural network th at was trained on 1.2 million non-\n",
      "medical images form the ImageNet2012 database with 1000 cla sses, and has been successfully\n",
      "employedasa genericfeatureextractorindifferentapplic ations.\n",
      "For the purposeof this experiment,we selected the fastOverFeat instantiation and used the default\n",
      "settingforfeatureextraction. Thisresultsina4096dimen sionalrepresentationoftheimages,which\n",
      "was subsequently used as input for the SVM classier. The regu larizationparameter C of the linear\n",
      "SVM wasestimatedonthe trainingset usingexhaustivesearc hand5-foldcross-validation.\n",
      "For the second direction we designed a network 2 inspired by t he work of Razavian et al. [7],\n",
      "using dropout and batch normalization with leaky ReLU activ ation functions [2]. In this case for\n",
      "trainingweusedimagesof128by128pixelswithvariousaugm entationsrangingfromresizingand\n",
      "cropping, rotation, translation, shearing, stretching an d ﬂipping. The results of the two directions\n",
      "aregivenintable 1.\n",
      "Table1: Results: Accuracyin percent\n",
      "OverFeat+ SVM CustomCNN\n",
      "92.42 95.14\n",
      "3 Conclusions and Perspectives\n",
      "Weinvestigatedwhen,where,andhowto(re)trainaconvolut ionalneuralnetworkusedforanatomy\n",
      "detection. Our results suggest that pretrained networks ar e good image descriptors even outside\n",
      "theirtrainingdomainandthatretrainingonlythelastlaye risaviablealternativetoperformtransfer\n",
      "learning in light of limited training data. We also demonstr ated that sufﬁciently expressive models\n",
      "such as a very deep neural network can be trained even on relat ively small number of annotations\n",
      "if properaugmentationand regularizationis implemented. Our work is a ﬁrst step towardsa multi-\n",
      "modalityanatomicalexpertsystemwith componentstrained tomaximizedataefﬁciency.\n",
      "References\n",
      "[1] C. CortesandV. Vapnik. Support-vectornetworks. Machinelearning ,20(3):273–297,1995.\n",
      "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti ﬁers: Surpassing human-level\n",
      "performanceonimagenetclassiﬁcation. arXivpreprintarXiv:1502.01852 ,2015.\n",
      "[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jai tly, A. Senior, V. Vanhoucke,\n",
      "P. Nguyen,T. N. Sainath,et al. Deepneuralnetworksforacou stic modelinginspeechrecogni-\n",
      "tion: The shared views of four research groups. SignalProcessing Magazine, IEEE , 29(6):82–\n",
      "97,2012.\n",
      "2http://www.imageclef.org/2009/medanno\n",
      "2\n",
      "[4] S. Ioffeand C. Szegedy. Batch normalization: Accelerat ingdeep networktrainingby reducing\n",
      "internalcovariateshift. arXivpreprintarXiv:1502.03167 ,2015.\n",
      "[5] A. Krizhevsky,I. Sutskever,and G. E. Hinton. Imagenetc lassiﬁcation with deepconvolutional\n",
      "neuralnetworks. In AdvancesinNeuralInformationProcessingSystems25(NIPS ),2012.\n",
      "[6] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, an d Y. LeCun. Overfeat: Integrated\n",
      "recognition, localization and detection using convolutio nalnetworks. In International Confer-\n",
      "enceonLearningRepresentations(ICLR2014) ,2014.\n",
      "[7] A.SharifRazavian,J.Sullivan,A.Maki,andS.Carlsson .Abaselineforvisualinstanceretrieval\n",
      "with deep convolutional networks. In International Conference on Learning Representations,\n",
      "May 7-9,2015,SanDiego,CA .ICLR, 2015.\n",
      "[8] N.Srivastava,G.Hinton,A.Krizhevsky,I.Sutskever,a ndR.Salakhutdinov. Dropout: Asimple\n",
      "way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research ,\n",
      "15(1):1929–1958,2014.\n",
      "[9] Y. B. YannLeCunandG.Hinton. Deeplearning. Nature,521:436–444,2015.\n",
      "Annex\n",
      "Table2: CustomDeepNeuralNetworkarchitecture\n",
      "ConvLayer+ BN (3x3,32x)\n",
      "ConvLayer+ BN (3x3,16x)\n",
      "MaxPoolLayer (3x3,2x2stride)\n",
      "ConvLayer+ BN (3x3,64x)\n",
      "ConvLayer+ BN (3x3,32x)\n",
      "MaxPoolLayer (3x3,2x2stride)\n",
      "ConvLayer+ BN (3x3,128)\n",
      "ConvLayer+ BN (3x3,128)\n",
      "ConvLayer+ BN (3x3,64)\n",
      "MaxPoolLayer (3x3,2x2stride)\n",
      "ConvLayer+ BN (3x3,256)\n",
      "ConvLayer+ BN (3x3,256)\n",
      "ConvLayer+ BN (3x3,128)\n",
      "MaxPoolLayer+Dropout (3x3,2x2stride)\n",
      "DenseLayer+ BN +Dropout (256)\n",
      "DenseLayer+ BN +Dropout (256)\n",
      "Softmax (24)\n",
      "3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "On constate :\n",
    "* la première page, les mots du titre sont collés, \n",
    "* de même pour les noms d'auteurs et les laboratoires,\n",
    "* les indices des notes de bas de page sont bons (à la taille près),\n",
    "* les références apparaissent proprement, mais avec un 2 avant [4], c'est le numéro de page,\n",
    "* le texte qui termine le document est le tableau qui termine l'article, suivi du numero de la page.\n",
    "\n",
    "\n",
    "Testons l'OCR par Tesseract :"
   ],
   "metadata": {
    "id": "6jR1a1dbxWgG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install PyMuPDF  # pour transformer les pdf en images"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4n1Dc8hpcHLS",
    "outputId": "3e1b9aac-c3ec-4847-c836-87ec73dc63e9"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.21.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.0/14.0 MB\u001B[0m \u001B[31m70.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.21.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import fitz\n",
    "\n",
    "# Chemin du fichier PDF à convertir en images\n",
    "pdf_file = filename\n",
    "\n",
    "# Configuration de la résolution et du format d'image\n",
    "resolution = 600\n",
    "image_format = 'png'\n",
    "images = []\n",
    "\n",
    "# Conversion du PDF en images\n",
    "with fitz.open(pdf_file) as pdf:\n",
    "    for i, page in enumerate(pdf):\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(resolution/72, resolution/72))  #getPixmap\n",
    "        num = str(i+1).zfill(2)\n",
    "        pix.save(f'page_{num}.{image_format}')\n",
    "        images.append(f'page_{num}.{image_format}')"
   ],
   "metadata": {
    "id": "VbHOleZEcHN6"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt install tesseract-ocr\n",
    "!pip install pytesseract"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRZFsi9gmJqG",
    "outputId": "eeca11f3-7790-4fc3-942b-4161287a1b38"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  tesseract-ocr-eng tesseract-ocr-osd\n",
      "The following NEW packages will be installed:\n",
      "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
      "0 upgraded, 3 newly installed, 0 to remove and 24 not upgraded.\n",
      "Need to get 4,850 kB of archives.\n",
      "After this operation, 16.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1 [1,598 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1 [2,990 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 tesseract-ocr amd64 4.1.1-2build2 [262 kB]\n",
      "Fetched 4,850 kB in 1s (5,150 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package tesseract-ocr-eng.\n",
      "(Reading database ... 122349 files and directories currently installed.)\n",
      "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
      "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
      "Selecting previously unselected package tesseract-ocr-osd.\n",
      "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1_all.deb ...\n",
      "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
      "Selecting previously unselected package tesseract-ocr.\n",
      "Preparing to unpack .../tesseract-ocr_4.1.1-2build2_amd64.deb ...\n",
      "Unpacking tesseract-ocr (4.1.1-2build2) ...\n",
      "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1) ...\n",
      "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1) ...\n",
      "Setting up tesseract-ocr (4.1.1-2build2) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (23.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from pytesseract) (8.4.0)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.10\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pytesseract\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "try:\n",
    " from PIL import Image\n",
    "except ImportError:\n",
    " import Image"
   ],
   "metadata": {
    "id": "4iiBGF9fmKD3"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\n",
    "for imageName in images:\n",
    "    # Extraction du texte de l'image avec Tesseract\n",
    "    ####img = Image.fromarray(page)\n",
    "    extracted_text = pytesseract.image_to_string(imageName, lang='eng')\n",
    "\n",
    "    # Ajout du texte extrait à la variable de stockage\n",
    "    text += extracted_text\n",
    "\n",
    "# Affichage du texte extrait\n",
    "print(text)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UybUiv08cHQ7",
    "outputId": "a242d895-4bd4-47a1-b964-c53d982cbbd9"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \n",
      "\n",
      "Can Pretrained Neural Networks Detect Anatomy?\n",
      "\n",
      "Vlado Menkovski Zharko Aleksovski\n",
      "Philips Research Philips Research\n",
      "Eindhoven, Netherlands Eindhoven, Netherlands\n",
      "vlado.menkovski@philips.com zharko.aleksovski@philips.com\n",
      "\n",
      "Axel Saalbach Hannes Nickisch\n",
      "Philips Research Philips Research\n",
      "\n",
      "Hamburg, Germany Hamburg, Germany\n",
      "\n",
      "axel.saalbach@philips.coom hannes.nickisch@philips.com\n",
      "Abstract\n",
      "\n",
      "Convolutional neural networks demonstrated outstanding empirical results in\n",
      "computer vision and speech recognition tasks where labeled training data is abun-\n",
      "dant. In medical imaging, there is a huge variety of possible imaging modalities\n",
      "and contrasts, where annotated data is usually very scarce. We present two ap-\n",
      "proaches to deal with this challenge. A network pretrained in a different domain\n",
      "with abundant data is used as a feature extractor, while a subsequent classifier is\n",
      "trained on a small target dataset; and a deep architecture trained with heavy aug-\n",
      "mentation and equipped with sophisticated regularization methods. We test the\n",
      "approaches on a corpus of X-ray images to design an anatomy detection system.\n",
      "\n",
      "1 Introduction and Motivation\n",
      "\n",
      "Deep learning [9] (DL) is rapidly gaining momentum in computer vision [5], speech recognition\n",
      "[3] and medical imaging applications due to their record-breaking empirical performance and their\n",
      "promise to jointly learn the low-level features and the high-level decisions directly from raw data.\n",
      "\n",
      "One major open research challenge is the transfer of the excellent predictive performance of DL\n",
      "methods, in particular convolutional neural networks (CNN), from the “data-laden” regime of com-\n",
      "puter vision! to the “data-bounded” regime of medical imaging.\n",
      "\n",
      "Since the typical number of parameters in the DL models is very big the major concern for operat-\n",
      "ing in “data-bounded” regimes is inability to generalize and form robust features because the small\n",
      "number of examples do not capture well the variability in the input space. There are various ap-\n",
      "proaches to deal with this challenge. Data augmentation can be used to increase the effective size\n",
      "of the training set and drive the training to build invariances to specific transformations. Network\n",
      "architecture design can be used to hard-code prior knowledge (such as translation invariance) by de-\n",
      "sign. Sophisticated regularization methods such as dropout [8] and batch normalization (BN) [4] can\n",
      "work to counter overfitting. Finally, global training of the feature extraction layers on abundant data\n",
      "domains (i.e. natural images) and then training the “classification” higher layers from the limited\n",
      "data domains can be used to ensure that the lower-level features are invariant to the particularities of\n",
      "the training dataset.\n",
      "\n",
      "We choose two directions to investigate these approaches for the goal of anatomy detection in X-ray\n",
      "images. The first using a network trained on a large corpus of natural images for feature extraction\n",
      "\n",
      "'There are 10° images in 10° classes in the ILSVRC challenge,\n",
      "http://www. image-net.org/challenges/LSVRC.\n",
      "\fcombined with a SVM [1] classifier trained on the medical images. And a second approach using\n",
      "heavy augmentation and regularization of a deep neural network trained only on the small-sized\n",
      "medical image dataset.\n",
      "\n",
      "2 Experiments and Results\n",
      "\n",
      "In order to assess the performance of specifically trained and off-the-shelf networks, radiographs\n",
      "from the ImageClef 2009 - Medical Image Annotation task* were used. This challenge database\n",
      "consists of a broad range of X-ray images from clinical routine, with a detailed anatomical classifi-\n",
      "cation. For convenience, we flattened the hierarchical class representation and disregarded classes\n",
      "with fewer than 50 example images. In doing so we ended up with 24 unique classes. For evaluation\n",
      "purposes, the entire data corpus (of 14676 images) was divided into a training and test set, covering\n",
      "90% and 10% of the data respectively.\n",
      "\n",
      "As a first baseline, the OverFeat network (see [6]) in combination with a (linear) multi-class SVM\n",
      "was employed. OverFeat is a convolutional neural network that was trained on 1.2 million non-\n",
      "\n",
      "medical images form the ImageNet2012 database with 1000 classes, and has been successfully\n",
      "employed as a generic feature extractor in different applications.\n",
      "\n",
      "For the purpose of this experiment, we selected the fast OverFeat instantiation and used the default\n",
      "setting for feature extraction. This results in a 4096 dimensional representation of the images, which\n",
      "was subsequently used as input for the SVM classier. The regularization parameter C of the linear\n",
      "SVM was estimated on the training set using exhaustive search and 5-fold cross-validation.\n",
      "\n",
      "For the second direction we designed a network 2 inspired by the work of Razavian et al. [7],\n",
      "using dropout and batch normalization with leaky ReLU activation functions [2]. In this case for\n",
      "training we used images of 128 by 128 pixels with various augmentations ranging from resizing and\n",
      "cropping, rotation, translation, shearing, stretching and flipping. The results of the two directions\n",
      "are given in table 1.\n",
      "\n",
      "Table 1: Results: Accuracy in percent\n",
      "OverFeat+SVM Custom CNN\n",
      "\n",
      "92.42 95.14\n",
      "\n",
      "3 Conclusions and Perspectives\n",
      "\n",
      "We investigated when, where, and how to (re)train a convolutional neural network used for anatomy\n",
      "detection. Our results suggest that pretrained networks are good image descriptors even outside\n",
      "their training domain and that retraining only the last layer is a viable alternative to perform transfer\n",
      "learning in light of limited training data. We also demonstrated that sufficiently expressive models\n",
      "such as a very deep neural network can be trained even on relatively small number of annotations\n",
      "if proper augmentation and regularization is implemented. Our work is a first step towards a multi-\n",
      "modality anatomical expert system with components trained to maximize data efficiency.\n",
      "\n",
      "References\n",
      "\n",
      "[1] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273—297, 1995.\n",
      "\n",
      "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level\n",
      "performance on imagenet classification. arXiv preprint arXiv: 1502.01852, 2015.\n",
      "\n",
      "[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke,\n",
      "P. Nguyen, T. N. Sainath, et al. Deep neural networks for acoustic modeling 1n speech recogni-\n",
      "\n",
      "tion: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82-\n",
      "97, 2012.\n",
      "\n",
      "\"http: //www.imageclef.org/2009/medanno\n",
      "\f[4] S. loffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\n",
      "internal covariate shift. arXiv preprint arXiv: 1502.03167, 2015.\n",
      "\n",
      "[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional\n",
      "neural networks. In Advances in Neural Information Processing Systems 25 (NIPS), 2012.\n",
      "\n",
      "[6] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated\n",
      "recognition, localization and detection using convolutional networks. In International Confer-\n",
      "ence on Learning Representations (ICLR 2014), 2014.\n",
      "\n",
      "[7] A. Sharif Razavian, J. Sullivan, A. Maki, and S. Carlsson. A baseline for visual instance retrieval\n",
      "\n",
      "with deep convolutional networks. In International Conference on Learning Representations,\n",
      "May 7-9, 2015, San Diego, CA. ICLR, 2015.\n",
      "\n",
      "[8] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple\n",
      "\n",
      "way to prevent neural networks from overfitting. The Journal of Machine Learning Research,\n",
      "15(1):1929-1958, 2014.\n",
      "\n",
      "[9] Y. B. Yann LeCun and G. Hinton. Deep learning. Nature, 521:436—-444, 2015.\n",
      "\n",
      "Annex\n",
      "\n",
      "Table 2: Custom Deep Neural Network architecture\n",
      "\n",
      "ConvLayer + BN (3x3, 32x)\n",
      "ConvLayer + BN (3x3, 16x)\n",
      "MaxPoolLayer (3x3, 2x2 stride)\n",
      "ConvLayer + BN (3x3, 64x)\n",
      "ConvLayer + BN (3x3, 32x)\n",
      "MaxPoolLayer (3x3, 2x2 stride)\n",
      "ConvLayer + BN (3x3, 128)\n",
      "ConvLayer + BN (3x3, 128)\n",
      "ConvLayer + BN (3x3, 64)\n",
      "MaxPoolLayer (3x3, 2x2 stride)\n",
      "ConvLayer + BN (3x3, 256)\n",
      "ConvLayer + BN (3x3, 256)\n",
      "ConvLayer + BN (3x3, 128)\n",
      "MaxPoolLayer + Dropout (3x3, 2x2 stride)\n",
      "DenseLayer + BN + Dropout (256)\n",
      "DenseLayer + BN + Dropout (256)\n",
      "Softmax (24)\n",
      "\f\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sur cet exemple :\n",
    "* les indices sont perdus, typiquement pour les notes de bas de page;\n",
    "* les exposants egalement\n",
    "* les url sont partiellement reconnues. \n",
    "\n",
    "Une recherche internet permet de vérifier que les \"superscripts & subscripts\" doivent faire l'objet d'un entrainement particulier.\n",
    "\n",
    "Tesseract est neanmoins un bon moyen pour corriger les problèmes de lecture des pdf, mais une lecture PDF demeure utile (on peut par exemple souvent lire toutes les url sans erreur).\n",
    "\n",
    "\n",
    "Le seul moyen, par exemple pour lire la bibliographie sans polution par les numéros de page, les notes de bas de page, voire même les en-têtes, est de passer par le fichier .tex. Dans notre cas, il est donnée au lien suivant https://arxiv.org/e-print/1512.05986v1. (Via le lien \"autres\" de la page d'accueil). La difficulté est qu'un peu plus de 15 % des articles ne fournissent pas un tel lien (statistique sur les articles CS.AI fin 2023).\n"
   ],
   "metadata": {
    "id": "IiL_ziEfzrwN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test sur un fichier recent\n"
   ],
   "metadata": {
    "id": "KRmqfnFDze-u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Au moment de la rédaction de ce notebook, le fichier le plus récent est http://arxiv.org/abs/2303.18246v1."
   ],
   "metadata": {
    "id": "Au2qOyFip_-A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "search = arxiv.Search(\n",
    "    #query = \"cat:cs.AI\", # les articles de la catégorie computer science et de la sous famille AI\n",
    "    query = \"2303.18246\", # pour avoir le dernier article cs.AI commentez cette ligne et utilisez celle du dessus.\n",
    "    max_results = 1, # nous n'allons utiliser ici qu'un seul fichier\n",
    "    sort_by = arxiv.SortCriterion.SubmittedDate # nous allons prendre le plus récent\n",
    ")\n",
    "for result in search.results():\n",
    "    print(result.entry_id) # A url http://arxiv.org/abs/{id}.\n",
    "    print(result.updated) # When the result was last updated.\n",
    "    print(result.published) # When the result was originally published.\n",
    "    print(result.title) # The title of the result.\n",
    "    print(result.authors) # The result's authors, as arxiv.Authors.\n",
    "    print(result.summary) # The result abstract.\n",
    "    print(result.comment) # The authors' comment if present.\n",
    "    print(result.journal_ref) # A journal reference if present.\n",
    "    print(result.doi) # A URL for the resolved DOI to an external resource if present.\n",
    "    print(result.primary_category) # The result's primary arXiv category. See arXiv: Category Taxonomy.\n",
    "    print(result.categories) # All of the result's categories. See arXiv: Category Taxonomy.\n",
    "    print(result.links) # Up to three URLs associated with this result, as arxiv.Links.\n",
    "    print(result.pdf_url) # "
   ],
   "metadata": {
    "id": "qHrg59gIq3k0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7608a18c-e12e-4039-facf-88e0881614d9"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://arxiv.org/abs/2303.18246v2\n",
      "2023-04-06 17:59:20+00:00\n",
      "2023-03-31 17:59:09+00:00\n",
      "3D Human Pose Estimation via Intuitive Physics\n",
      "[arxiv.Result.Author('Shashank Tripathi'), arxiv.Result.Author('Lea Müller'), arxiv.Result.Author('Chun-Hao P. Huang'), arxiv.Result.Author('Omid Taheri'), arxiv.Result.Author('Michael J. Black'), arxiv.Result.Author('Dimitrios Tzionas')]\n",
      "Estimating 3D humans from images often produces implausible bodies that lean,\n",
      "float, or penetrate the floor. Such methods ignore the fact that bodies are\n",
      "typically supported by the scene. A physics engine can be used to enforce\n",
      "physical plausibility, but these are not differentiable, rely on unrealistic\n",
      "proxy bodies, and are difficult to integrate into existing optimization and\n",
      "learning frameworks. In contrast, we exploit novel intuitive-physics (IP) terms\n",
      "that can be inferred from a 3D SMPL body interacting with the scene. Inspired\n",
      "by biomechanics, we infer the pressure heatmap on the body, the Center of\n",
      "Pressure (CoP) from the heatmap, and the SMPL body's Center of Mass (CoM). With\n",
      "these, we develop IPMAN, to estimate a 3D body from a color image in a \"stable\"\n",
      "configuration by encouraging plausible floor contact and overlapping CoP and\n",
      "CoM. Our IP terms are intuitive, easy to implement, fast to compute,\n",
      "differentiable, and can be integrated into existing optimization and regression\n",
      "methods. We evaluate IPMAN on standard datasets and MoYo, a new dataset with\n",
      "synchronized multi-view images, ground-truth 3D bodies with complex poses,\n",
      "body-floor contact, CoM and pressure. IPMAN produces more plausible results\n",
      "than the state of the art, improving accuracy for static poses, while not\n",
      "hurting dynamic ones. Code and data are available for research at\n",
      "https://ipman.is.tue.mpg.de.\n",
      "Accepted in CVPR'23. Project page: https://ipman.is.tue.mpg.de\n",
      "None\n",
      "None\n",
      "cs.CV\n",
      "['cs.CV', 'cs.AI', 'cs.GR']\n",
      "[arxiv.Result.Link('http://arxiv.org/abs/2303.18246v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2303.18246v2', title='pdf', rel='related', content_type=None)]\n",
      "http://arxiv.org/pdf/2303.18246v2\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filename2 = './'+result.pdf_url[21:]+'.pdf'\n",
    "\n",
    "# Envoi d'une requête pour récupérer le contenu du PDF\n",
    "response = requests.get(result.pdf_url)\n",
    "\n",
    "# Vérification du statut de la réponse\n",
    "if response.status_code == 200:\n",
    "    # Récupération du contenu du PDF et enregistrement dans un fichier local   \n",
    "    with open(filename2, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "        print('PDF téléchargé avec succès !')\n",
    "else:\n",
    "    print('Impossible de télécharger le PDF. Code d\\'erreur:', response.status_code)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KfTnV8qq3nL",
    "outputId": "dae4aaf9-cdf8-414d-c341-d62e9607c435"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PDF téléchargé avec succès !\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "page_content = \"\"\n",
    "with open(filename2, \"rb\") as pdf_file:\n",
    "    read_pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    number_of_pages = len(read_pdf.pages)\n",
    "    for i in range(number_of_pages):\n",
    "      page = read_pdf.pages[i]\n",
    "      page_content = page_content +'\\n' + page.extract_text(orientations=0)\n",
    "\n",
    "print(page_content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EipxbKZq3p0",
    "outputId": "cae7420b-7a50-4696-bc6e-dc742055c710"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "3D Human Pose Estimation via Intuitive Physics\n",
      "Shashank Tripathi1Lea M ¨uller1Chun-Hao P. Huang1Omid Taheri1\n",
      "Michael J. Black1Dimitrios Tzionas2*\n",
      "1Max Planck Institute for Intelligent Systems, T ¨ubingen, Germany2University of Amsterdam, the Netherlands\n",
      "{stripathi, lmueller2, chuang2, otaheri, black }@tue.mpg.de d.tzionas@uva.nl\n",
      "Unstable Poses\n",
      "Floor Penetration Floating Bodies\n",
      "Figure 1. Estimating a 3D body from an image is ill-posed. A recent, representative, optimization method [64] produces bodies that are in\n",
      "unstable poses, penetrate the floor, or hover above it. In contrast, IPMAN estimates a 3D body that is physically plausible . To achieve this,\n",
      "IPMAN uses novel intuitive-physics (IP) terms that exploit inferred pressure heatmaps on the body, the Center of Pressure (CoP), and the\n",
      "body’s Center of Mass (CoM). Body heatmap colors encode per-vertex pressure.\n",
      "Abstract\n",
      "Estimating 3D humans from images often produces im-\n",
      "plausible bodies that lean, float, or penetrate the floor. Such\n",
      "methods ignore the fact that bodies are typically supported\n",
      "by the scene. A physics engine can be used to enforce phys-\n",
      "ical plausibility, but these are not differentiable, rely on\n",
      "unrealistic proxy bodies, and are difficult to integrate into ex-\n",
      "isting optimization and learning frameworks. In contrast, we\n",
      "exploit novel intuitive-physics (IP) terms that can be inferred\n",
      "from a 3D SMPL body interacting with the scene. Inspired\n",
      "by biomechanics, we infer the pressure heatmap on the body,\n",
      "theCenter of Pressure (CoP) from the heatmap, and the\n",
      "SMPL body’s Center of Mass (CoM ). With these, we develop\n",
      "IPMAN , to estimate a 3D body from a color image in a “sta-\n",
      "ble” configuration by encouraging plausible floor contact\n",
      "and overlapping CoP andCoM . Our IP terms are intuitive,\n",
      "easy to implement, fast to compute, differentiable, and can be\n",
      "integrated into existing optimization and regression methods.\n",
      "We evaluate IPMAN on standard datasets and MoYo, a new\n",
      "dataset with synchronized multi-view images, ground-truth\n",
      "3D bodies with complex poses, body-floor contact, CoM and\n",
      "pressure. IPMAN produces more plausible results than the\n",
      "state of the art, improving accuracy for static poses, while\n",
      "not hurting dynamic ones. Code and data are available for\n",
      "research at https://ipman.is.tue.mpg.de .\n",
      "*This work was mostly performed at MPI-IS.1. Introduction\n",
      "To understand humans and their actions, computers need\n",
      "automatic methods to reconstruct the body in 3D. Typi-\n",
      "cally, the problem entails estimating the 3D human pose\n",
      "and shape ( HPS) from one or more color images. State-of-\n",
      "the-art ( SOTA ) methods [47, 53, 80, 110] have made rapid\n",
      "progress, estimating 3D humans that align well with image\n",
      "features in the camera view. Unfortunately, the camera view\n",
      "can be deceiving. When viewed from other directions, or\n",
      "when placed in a 3D scene, the estimated bodies are often\n",
      "physically implausible: they lean, hover, or penetrate the\n",
      "ground (see Fig. 1 top). This is because most SOTA methods\n",
      "reason about humans in isolation ; they ignore that people\n",
      "move in a scene, interact with it, and receive physical sup-\n",
      "port by contacting it. This is a deal-breaker for inherently\n",
      "3D applications, such as biomechanics, augmented/virtual\n",
      "reality (AR/VR) and the “metaverse”; these need humans to\n",
      "be reconstructed faithfully and physically plausibly with re-\n",
      "spect to the scene. For this, we need a method that estimates\n",
      "the 3D human on a ground plane from a color image in a\n",
      "configuration that is physically “stable” .\n",
      "This is naturally related to reasoning about physics and\n",
      "support. There exist many physics simulators [10,30,65] for\n",
      "games, movies, or industrial simulations, and using these for\n",
      "plausible HPS inference is increasingly popular [71,79,104].\n",
      "However, existing simulators come with two significant\n",
      "1\n",
      "problems: (1) They are typically non-differentiable black\n",
      "boxes , making them incompatible with existing optimiza-\n",
      "tion and learning frameworks. Consequently, most meth-\n",
      "ods [69, 103, 104] use them with reinforcement learning to\n",
      "evaluate whether a certain input has the desired outcome,\n",
      "but with no ability to reason about how changing inputs af-\n",
      "fects the outputs. (2) They rely on an unrealistic proxy body\n",
      "model for computational efficiency; bodies are represented\n",
      "as groups of rigid 3D shape primitives. Such proxy models\n",
      "are crude approximations of human bodies, which, in reality,\n",
      "are much more complex and deform non-rigidly when they\n",
      "move and interact. Moreover, proxies need a priori known\n",
      "body dimensions that are kept fixed during simulation. Also,\n",
      "these proxies differ significantly from the 3D body mod-\n",
      "els [42, 56, 99] used by SOTA HPS methods. Thus, current\n",
      "physics simulators are too limited for use in HPS.\n",
      "What we need, instead, is a solution that is fully differen-\n",
      "tiable, uses a realistic body model, and seamlessly integrates\n",
      "physical reasoning into HPS methods (both optimization-\n",
      "and regression-based). To this end, instead of using full\n",
      "physics simulation, we introduce novel intuitive-physics (IP)\n",
      "terms that are simple, differentiable, and compatible with a\n",
      "body model like SMPL [56]. Specifically, we define terms\n",
      "that exploit an inferred pressure heatmap of the body on the\n",
      "ground plane, the Center of Pressure (CoP) that arises from\n",
      "the heatmap, and the SMPL body’s Center of Mass (CoM )\n",
      "projected on the floor; see Fig. 2 for a visualization. Intu-\n",
      "itively, bodies whose CoM lie close to their CoP are more\n",
      "stable than ones with a CoP that is further away (see Fig. 5);\n",
      "the former suggests a static pose ,e.g.standing or holding a\n",
      "yoga pose, while the latter a dynamic pose , e.g., walking.\n",
      "We use these intuitive-physics terms in two ways. First,\n",
      "we incorporate them in an objective function that extends\n",
      "SMPLify-XMC [64] to optimize for body poses that are\n",
      "stable. We also incorporate the same terms in the training\n",
      "loss for an HPS regressor, called IPMAN (Intuitive-Physics-\n",
      "based huMAN). In both formulations, the intuitive-physics\n",
      "terms encourage estimates of body shape and pose that have\n",
      "sufficient ground contact, while penalizing interpenetration\n",
      "and encouraging an overlap of the CoP and CoM.\n",
      "Our intuitive-physics formulation is inspired by work in\n",
      "biomechanics [32, 33, 66], which characterizes the stability\n",
      "of humans in terms of relative positions between the CoP,\n",
      "theCoM , and the Base of Support (BoS). The BoS is de-\n",
      "fined as the convex hull of all contact regions on the floor\n",
      "(Fig. 2). Following past work [6,76,79], we use the “inverted\n",
      "pendulum” model [92, 93] for body balance; this considers\n",
      "poses as stable if the gravity-projected CoM onto the floor\n",
      "lies inside the BoS. Similar ideas are explored by Scott et\n",
      "al. [76] but they focus on predicting a foot pressure heatmap\n",
      "from 2D or 3D body joints. We go significantly further to\n",
      "exploit stability in training an HPS regressor. This requires\n",
      "two technical novelties.\n",
      "CoM \n",
      "BoS \n",
      "CoP \n",
      "Figure 2. (1) A SMPL mesh sitting. (2) The inferred pressure map\n",
      "on the ground (color-coded heatmap), CoP (green), CoM (pink),\n",
      "and Base of Support ( BoS, yellow polygon). (3) Segmentation of\n",
      "SMPL intoNP= 10 parts, used for computing CoM ; see Sec. 3.2.\n",
      "The first involves computing CoM . To this end, we uni-\n",
      "formly sample points on SMPL ’s surface, and calculate each\n",
      "body part’s volume. Then, we compute CoM as the average\n",
      "of all uniformly sampled points weighted by the correspond-\n",
      "ing part volumes. We denote this as pCoM , standing for\n",
      "“part-weighted CoM ”. Importantly, pCoM takes into account\n",
      "SMPL ’s shape, pose, and all blend shapes, while it is also\n",
      "computationally efficient and differentiable.\n",
      "The second involves estimating CoP directly from the\n",
      "image, without access to a pressure sensor. Our key insight is\n",
      "that the soft tissues of human bodies deform under pressure,\n",
      "e.g., the buttocks deform when sitting. However, SMPL\n",
      "does not model this deformation; it penetrates the ground\n",
      "instead of deforming. We use the penetration depth as a\n",
      "proxy for pressure [73]; deeper penetration means higher\n",
      "pressure. With this, we estimate a pressure field on SMPL ’s\n",
      "mesh and compute the CoP as the pressure-weighted average\n",
      "of the surface points. Again this is differentiable.\n",
      "For evaluation, we use a standard HPS benchmark\n",
      "(Human3.6M [37]), but also the RICH [35] dataset. How-\n",
      "ever, these datasets have limited interactions with the floor.\n",
      "We thus capture a novel dataset, MoYo , of challenging yoga\n",
      "poses, with synchronized multi-view video, ground-truth\n",
      "SMPL-X [68] meshes, pressure sensor measurements, and\n",
      "body CoM .IPMAN , in both of its forms, and across all\n",
      "datasets, produces more accurate and stable 3D bodies than\n",
      "the state of the art. Importantly, we find that IPMAN im-\n",
      "proves accuracy for static poses, while not hurting dynamic\n",
      "ones. This makes IPMAN applicable to everyday motions.\n",
      "To summarize: (1) We develop IPMAN , the first HPS\n",
      "method that integrates intuitive physics. (2) We infer biome-\n",
      "chanical properties such as CoM ,CoP and body pressure.\n",
      "(3) We define novel intuitive-physics terms that can be easily\n",
      "integrated into HPS methods. (4) We create MoYo , a dataset\n",
      "that uniquely has complex poses, multi-view video, and\n",
      "ground-truth bodies, pressure, and CoM . (5) We show that\n",
      "our IP terms improve HPS accuracy and physical plausibility.\n",
      "(6) Data and code are available for research.\n",
      "2. Related Work\n",
      "3D Human Pose and Shape (HPS) from images. Ex-\n",
      "isting methods fall into two major categories: (1) non-\n",
      "2\n",
      "parametric methods that reconstruct a free-form body rep-\n",
      "resentation, e.g., joints [1, 61, 62] or vertices [54, 63, 108],\n",
      "and (2) parametric methods that use statistical body mod-\n",
      "els [5, 25, 42, 56, 68, 99, 105]. The latter methods focus on\n",
      "various aspects, such as expressiveness [13, 18, 68, 74, 94],\n",
      "clothed bodies [15, 95, 98], videos [24, 46, 83, 107], and\n",
      "multi-person scenarios [38, 80, 111], to name a few.\n",
      "Inference is done by either optimization or regression.\n",
      "Optimization-based methods [7, 16, 68, 94, 95] fit a body\n",
      "model to image evidence, such as joints [11], dense ver-\n",
      "tex correspondences [2] or 2D segmentation masks [23].\n",
      "Regression-based methods [43, 45, 50, 53, 81, 110, 114, 117]\n",
      "use a loss similar to the objective function of optimization\n",
      "methods to train a network to infer body model parameters.\n",
      "Several methods combine optimization and regression in\n",
      "a training loop [49, 52, 64]. Recent methods [24, 41] fine-\n",
      "tune pre-trained networks at test time w.r.t. an image or a\n",
      "sequence, retaining flexibility (optimization) while being\n",
      "less sensitive to initialization (regression).\n",
      "Despite their success, these methods reason about the\n",
      "human in “isolation”, without taking the surrounding scene\n",
      "into account; see [82, 115] for a comprehensive review.\n",
      "Contact-only scene constraints. A common way of\n",
      "using scene information is to consider body-scene con-\n",
      "tact[12, 17, 27, 28, 70, 91, 97, 101, 106, 112, 113, 118]. Ya-\n",
      "mamoto et al. [100] and others [19, 27, 75, 106, 112] en-\n",
      "sure that estimated bodies have plausible scene contact. For\n",
      "videos, encouraging foot-ground contact reduces foot skat-\n",
      "ing [36, 70, 77, 113, 118]. Weng et al. [91] use contact in\n",
      "estimating the pose and scale of scene objects, while Ville-\n",
      "gaset al. [87] preserve self- and ground contact for motion\n",
      "retargeting.\n",
      "These methods typically take two steps: (1) detecting\n",
      "contact areas on the body and/or scene and (2) minimizing\n",
      "the distance between these. Surfaces are typically assumed\n",
      "to be in contact if their distance is below a threshold and\n",
      "their relative motion is small [27, 35, 106, 112].\n",
      "Many methods only consider contact between the ground\n",
      "and the foot joints [71, 118] or other end-effectors [70]. In\n",
      "contrast, IPMAN uses the full 3D body surface and exploits\n",
      "this to compute the pressure, CoP andCoM . Unlike binary\n",
      "contact, this is differentiable, making the IP terms useful for\n",
      "training HPS regressors.\n",
      "Physics-based scene constraints. Early work uses\n",
      "physics to estimate walking [8, 9] or full body motion [89].\n",
      "Recent methods [21,22,71,78,79,96,104] regress 3D humans\n",
      "and then refine them through physics-based optimization .\n",
      "Physics is used for two primary reasons: (1) to regularise dy-\n",
      "namics, reducing jitter [51,71,79,104], and (2) to discourage\n",
      "interpenetration and encourage contact. Since contact events\n",
      "are discontinuous, the pipeline is either not end-to-end train-\n",
      "able or trained with reinforcement learning [69, 104]. Xie\n",
      "et al. [96] propose differentiable physics-inspired objectivesbased on a soft contact penalty, while DiffPhy [21] uses a\n",
      "differentiable physics simulator [31] during inference. Both\n",
      "methods apply the objectives in an optimization scheme,\n",
      "while IPMAN is applied to both optimization and regres-\n",
      "sion. PhysCap [79] considers a pose as balanced, when the\n",
      "CoM is projected within the BoS. Rempe et al. [71] impose\n",
      "PD control on the pelvis, which they treat as a CoM . Scott\n",
      "et al. [76] regress foot pressure from 2D and 3D joints for\n",
      "stability analysis but do not use it to improve HPS.\n",
      "All these methods use unrealistic bodies based on shape\n",
      "primitives. Some require known body dimensions [71, 79,\n",
      "104] while others estimate body scale [51, 96]. In contrast,\n",
      "IPMAN computes CoM ,CoP andBoS directly from the\n",
      "SMPL mesh. Clever et al. [14] and Luo et al. [58] estimate\n",
      "3D body pose but from pressure measurements, not from\n",
      "images. Their task is fundamentally different from ours.\n",
      "3. Method\n",
      "3.1. Preliminaries\n",
      "Given a color image, I, we estimate the parameters of the\n",
      "camera and the SMPL body model [56].\n",
      "Body model. SMPL maps pose, θ, and shape, β, pa-\n",
      "rameters to a 3D mesh, M(θ,β). The pose parameters,\n",
      "θ∈R24×6, are rotations of SMPL ’s 24 joints in a 6D rep-\n",
      "resentation [116]. The shape parameters, β∈R10, are\n",
      "the first 10 PCA coefficients of SMPL ’s shape space. The\n",
      "generated mesh M(θ,β)consists of NV= 6890 vertices,\n",
      "V∈RNV×3, and NF= 13776 faces, F∈RNF×3×3.\n",
      "Note that our regression method ( IPMAN-R , Sec. 3.4.1)\n",
      "uses SMPL , while our optimization method ( IPMAN-O ,\n",
      "Sec. 3.4.2) uses SMPL-X [68], to match the models used by\n",
      "the baselines. For simplicity of exposition, we refer to both\n",
      "models as SMPL when the distinction is not important.\n",
      "Camera. For the regression-based IPMAN-R , we follow\n",
      "the standard convention [43, 44, 49] and use a weak perspec-\n",
      "tive camera with a 2D scale, s, translation, tc= (tc\n",
      "x, tc\n",
      "y),\n",
      "fixed camera rotation, Rc=I3, and a fixed focal length\n",
      "(fx, fy). The root-relative body orientation Rbis predicted\n",
      "by the neural network, but body translation stays fixed at\n",
      "tb=0as it is absorbed into the camera’s translation.\n",
      "For the optimization-based IPMAN-O , we follow M ¨uller\n",
      "et al. [64] to use the full-perspective camera model and op-\n",
      "timize the focal lengths (fx, fy), camera rotation Rcand\n",
      "camera translation tc. The principal point (ox, oy)is the\n",
      "center of the input image. Kis the intrinsic matrix storing\n",
      "focal lengths and the principal point. We assume that the\n",
      "body rotation Rband translation tbare absorbed into the\n",
      "camera parameters, thus, they stay fixed as Rb=I3and\n",
      "tb=0. Using the camera, we project a 3D point X∈R3to\n",
      "an image point x∈R2through x=K(RcX+tc).\n",
      "Ground plane and gravity-projection. We assume that\n",
      "the gravity direction is perpendicular to the ground plane in\n",
      "3\n",
      "the world coordinate system. Thus, for any arbitrary point in\n",
      "3D space, u∈R3, itsgravity-projected point,u′=g(u)∈\n",
      "R3, is the projection of ualong the plane normal nonto\n",
      "the ground plane, and g(.)is the projection operator. The\n",
      "function h(u)returns the signed “height” of a point uwith\n",
      "respect to the ground; i.e., the signed distance from uto the\n",
      "ground plane along the gravity direction, where h(u)<0if\n",
      "uis below the ground and h(u)>0ifuis above it.\n",
      "3.2. Stability Analysis\n",
      "We follow the biomechanics literature [32, 33, 66] and\n",
      "Scott et al. [76] to define three fundamental elements for\n",
      "stability analysis: We use the Newtonian definition for the\n",
      "“Center of Mass” ( CoM );i.e., the mass-weighted average\n",
      "of particle positions. The “Center of Pressure” ( CoP) is\n",
      "the ground-reaction force’s point of application. The “Base\n",
      "of Support” ( BoS) is the convex hull of all body-ground\n",
      "contacts. Below, we define intuitive-physics (IP) terms using\n",
      "the inferred CoM andCoP.BoS is only used for evaluation.\n",
      "Body Center of Mass (CoM). We introduce a novel\n",
      "CoM formulation that is fully differentiable and consid-\n",
      "ers the per-part mass contributions, dubbed as pCoM ; see\n",
      "Sup. Mat. for alternative CoM definitions. To compute this,\n",
      "we first segment the template mesh into NP= 10 parts\n",
      "Pi∈ P; see Fig. 2. We do this once offline, and keep the\n",
      "segmentation fixed during training and optimization. Assum-\n",
      "ing a shaped and posed SMPL body, the per-part volumes\n",
      "VPiare calculated by splitting the SMPL mesh into parts.\n",
      "However, mesh splitting is a non-differentiable opera-\n",
      "tion. Thus, it cannot be used for either training a regressor\n",
      "(IPMAN-R ) or for optimization ( IPMAN-O ). Instead, we\n",
      "work with the full SMPL mesh and use differentiable “close-\n",
      "translate-fill” operations for each body part on the fly. First,\n",
      "for each part P, we extract boundary vertices BPand add in\n",
      "the middle a virtual vertex vg, where vg=P\n",
      "j∈BPvj/|BP|.\n",
      "Then, for the BPandvgvertices, we add virtual faces to\n",
      "“close” Pand make it watertight . Next, we “translate” P\n",
      "such that the part centroid cP=P\n",
      "j∈Pvj/|P|is at the ori-\n",
      "gin. Finally, we “fill” the centered Pwith tetrahedrons by\n",
      "connecting the origin with each face vertex. Then, the part\n",
      "volume, VP, is the sum of all tetrahedron volumes [109].\n",
      "To create a uniform distribution of surface vertices, we\n",
      "uniformly sample NU= 20000 surface points VU∈\n",
      "RNU×3on the template SMPL mesh using the Triangle\n",
      "Point Picking method [90]. Given VUand the template\n",
      "SMPL mesh vertices VT, we follow [64], and analytically\n",
      "compute a sparse linear regressor W∈RNU×NVsuch that\n",
      "VU=WVT. During training and optimization, given an\n",
      "arbitrary shaped and posed mesh with vertices V, we ob-\n",
      "tain uniformly-sampled mesh surface points as VU=WV.\n",
      "Each surface point, vi, is assigned to the body part, Pvi,\n",
      "corresponding to the face, Fvi, it was sampled from.\n",
      "Finally, the part-weighted pCoM is computed as avolume-weighted mean of the mesh surface points:\n",
      "¯m=PNU\n",
      "i=1VPviviPNU\n",
      "i=1VPvi, (1)\n",
      "whereVPviis the volume of the part Pvi∈ P to which viis\n",
      "assigned. This formulation is fully differentiable and can be\n",
      "employed with any existing 3D HPS estimation method.\n",
      "Note that computing CoM (or volume) from uniformly\n",
      "sampled surface points does not work (see Sup. Mat.) be-\n",
      "cause it assumes that mass, M, is proportional to surface\n",
      "area,S. Instead, our pCoM computes mass from volume, V,\n",
      "via the standard density equation, M=ρV, while our close-\n",
      "translate-fill operation computes the volume of deformable\n",
      "bodies in an efficient and differentiable manner.\n",
      "Center of Pressure (CoP). Recovering a pressure\n",
      "heatmap from an image without using hardware, such as\n",
      "pressure sensors, is a highly ill-posed problem. However,\n",
      "stability analysis requires knowledge of the pressure ex-\n",
      "erted on the human body by the supporting surfaces, like the\n",
      "ground. Going beyond binary contact, Rogez et al. [73] esti-\n",
      "mate 3D forces by detecting intersecting vertices between\n",
      "hand and object meshes. Clever et al. [14] recover pressure\n",
      "maps by allowing articulated body models to deform a soft\n",
      "pressure-sensing virtual mattress in a physics simulation.\n",
      "In contrast, we observe that, while real bodies interacting\n",
      "with rigid objects ( e.g., the floor) deform under contact,\n",
      "SMPL does not model such soft-tissue deformations. Thus,\n",
      "the body mesh penetrates the contacting object surface and\n",
      "the amount of penetration can be a proxy for pressure; a\n",
      "deeper penetration implies higher pressure. With the height\n",
      "h(vi)(see Sec. 3.1) of a mesh surface point viwith respect\n",
      "to the ground plane Π, we define a pressure field to compute\n",
      "the per-point pressure ρias:\n",
      "ρi=(\n",
      "1−αh(vi)ifh(vi)<0,\n",
      "e−γh(vi)ifh(vi)≥0,(2)\n",
      "where αandγare scalar hyperparameters set empirically.\n",
      "We approximate soft tissue via a “spring” model and “pene-\n",
      "trating” pressure field using Hooke’s Law. Some pressure is\n",
      "also assigned to points above the ground to allow tolerance\n",
      "for footwear, but this decays quickly. Finally, we compute\n",
      "the CoP, ¯ s, as\n",
      "¯ s=PNU\n",
      "i=1ρiviPNU\n",
      "i=1ρi. (3)\n",
      "Again, note that this term is fully differentiable.\n",
      "Base of Support (BoS). In biomechanics [34, 92], BoS\n",
      "is defined as the “supporting area” or the possible range of\n",
      "theCoP on the supporting surface. Here, we define BoS as\n",
      "the convex hull [72] of all gravity-projected body-ground\n",
      "contact points. In detail, we first determine all such contacts\n",
      "4\n",
      "by selecting the set of mesh surface points viclose to the\n",
      "ground, and then gravity-project them onto the ground to\n",
      "obtain C={g(vi)\f\f|h(vi)|< τ}. The BoS is then defined\n",
      "as the convex hull CofC.\n",
      "3.3. Intuitive-Physics Losses\n",
      "Stability loss. The“inverted pendulum” model of human\n",
      "balance [92,93] considers the relationship between the CoM\n",
      "andBoS to determine stability. Simply put, for a given shape\n",
      "and pose, if the body CoM , projected on the gravity-aligned\n",
      "ground plane, lies within the BoS, the pose is considered sta-\n",
      "ble. While this definition of stability is useful for evaluation,\n",
      "using it in a loss or energy function for 3D HPS estimation\n",
      "results in sparse gradients (see Sup. Mat.). Instead, we define\n",
      "the stability criterion as:\n",
      "Lstability =∥g(¯m)−g(¯s)∥2, (4)\n",
      "where g(¯m)andg(¯s)are the gravity-projected CoM and\n",
      "CoP, respectively.\n",
      "Ground contact loss. As shown in Fig. 1, 3D HPS meth-\n",
      "ods minimize the 2D joint reprojection error and do not\n",
      "consider the plausibility of body-ground contact. Ignoring\n",
      "this can result in interpenetrating or hovering meshes. In-\n",
      "spired by self-contact losses [19,64] and hand-object contact\n",
      "losses [26,29], we define two ground losses, namely pushing,\n",
      "Lpush, and pulling, Lpull, that take into account the height,\n",
      "h(vi), of a vertex, vi, with respect to the ground plane. For\n",
      "h(vi)<0,i.e., for vertices under the ground plane, Lpush\n",
      "discourages body-ground penetrations. For h(vi)≥0,i.e.,\n",
      "for hovering meshes, Lpullencourages the vertices that lie\n",
      "close to the ground to “snap” into contact with it. Note that\n",
      "the losses are non-conflicting as they act on disjoint sets of\n",
      "vertices. Then, the ground contact loss is:\n",
      "Lground =Lpull+Lpush, with (5)\n",
      "Lpull =α1tanh (h(vi)\n",
      "α2)2ifh(vi)≥0, and (6)\n",
      "Lpush =β1tanh (h(vi)\n",
      "β2)2ifh(vi)<0. (7)\n",
      "3.4. IPMAN\n",
      "We use our new IP losses for two tasks: (1) We ex-\n",
      "tend HMR [43] to develop IPMAN-R , a regression-based\n",
      "HPS method. (2) We extend SMPLify-XMC [64] to de-\n",
      "velop IPMAN-O , an optimization-based method. Note that\n",
      "IPMAN-O uses a reference ground plane, while IPMAN-R\n",
      "uses the ground plane only for training but not at test time.\n",
      "It leverages the known ground in 3D datasets, and thus, does\n",
      "not require additional data beyond past HPS methods.\n",
      "3.4.1 IPMAN-R\n",
      "Most HPS methods are trained with a mix of direct supervi-\n",
      "sion using 3D datasets [37,61,88] and 2D reprojection losses\n",
      "HMR\n",
      "Regressor \n",
      "Figure 3. IPMAN-R architecture. First, the HMR regressor esti-\n",
      "mates camera translation and SMPL parameters for an input image.\n",
      "These parameters are used to generate the SMPL mesh in the cam-\n",
      "era frame, Mc. To transform the mesh from camera into world\n",
      "coordinates ( Mc→Mw), IPMAN-R uses the ground-truth cam-\n",
      "era rotation, Rc\n",
      "w, and translation, tc\n",
      "w. The IP losses, Lground and\n",
      "Lstability , are applied on the mesh in the world coordinate system.\n",
      "using image datasets [4, 39, 55]. The 3D losses, however,\n",
      "are calculated in the camera frame, ignoring scene informa-\n",
      "tion and physics. IPMAN-R extends HMR [43] with our\n",
      "intuitive-physics terms; see Fig. 3 for the architecture. For\n",
      "training, we use the known camera coordinates and the world\n",
      "ground plane in 3D datasets.\n",
      "As described in Sec. 3.1 (paragraph “Camera”), HMR\n",
      "infers the camera translation, tc, and SMPL parameters, θ\n",
      "andβ, in the camera coordinates assuming Rc=I3and\n",
      "tb=0. Ground truth 3D joints and SMPL parameters are\n",
      "used to supervise the inferred mesh Mcin the camera frame .\n",
      "However, 3D datasets also provide the ground, albeit in the\n",
      "world frame. To leverage the known ground, we transform\n",
      "the predicted body orientation, Rb, to world coordinates us-\n",
      "ing the ground-truth camera rotation, Rc\n",
      "w, asRb\n",
      "w=Rc⊤\n",
      "wRb.\n",
      "Then, we compute the body translation in world coordinates\n",
      "astb\n",
      "w=−tc+tc\n",
      "w. With the predicted mesh and ground\n",
      "plane in world coordinates, we add the IP terms, Lstability and\n",
      "Lground , for HPS training as follows:\n",
      "LIPMAN-R (θ,β,tc) =λ2DL2D+λ3DL3D+λSMPLLSMPL+\n",
      "λsLstability +λgLground , (8)\n",
      "where λsandλgare the weights for the respective IP terms.\n",
      "For training (data augmentation, hyperparameters, etc), we\n",
      "follow Kolotouros et al. [49]; for more details see Sup. Mat.\n",
      "3.4.2 IPMAN-O\n",
      "To fit SMPL-X to 2D image keypoints, SMPLify-XMC [64]\n",
      "initializes the fitting process by exploiting the self-contact\n",
      "and global-orientation of a known/presented 3D mesh. We\n",
      "posit that the presented pose contains further information,\n",
      "such as stability, pressure and contact with the ground-plane.\n",
      "IPMAN-O uses this insight to apply stability and ground\n",
      "5\n",
      "contact losses. The IPMAN-O objective is:\n",
      "EIPMAN-O (β,θ,Φ) =EJ2D+λβEβ+λθhEθh+\n",
      "λ˜θbE˜θb+λ˜CE˜C+\n",
      "λsEstability +λgEground . (9)\n",
      "Φdenotes the camera parameters: rotation Rc, translation\n",
      "tc, and focal length, (fx, fy).EJ2Dis a 2D joint loss, Eβ\n",
      "andEθhareL2body shape and hand pose priors. E˜θband\n",
      "E˜Care pose and contact terms w.r.t. the presented 3D pose\n",
      "and contact (see [64] for details). ESandEGare the stability\n",
      "and ground contact losses from Sec. 3.3. Since the estimated\n",
      "mesh is in the same coordinate system as the presented mesh\n",
      "and the ground-plane, we directly apply IP losses without\n",
      "any transformations. For details see Sup. Mat.\n",
      "4. Experiments\n",
      "4.1. Training and Evaluation Datasets\n",
      "Human3.6M [37]. A dataset of 3D human keypoints and\n",
      "RGB images. The poses are limited in terms of challeng-\n",
      "ing physics, focusing on common activities like walking,\n",
      "discussing, smoking, or taking photos.\n",
      "RICH [35]. A dataset of videos with accurate marker-less\n",
      "motion-captured 3D bodies and 3D scans of scenes. The\n",
      "images are more natural than Human3.6M and Fit3D [20].\n",
      "We consider sequences with meaningful body-ground inter-\n",
      "action. For the list of sequences, see Sup. Mat.\n",
      "Other datasets. Similar to [49], for training we use 3D\n",
      "keypoints from MPI-INF-3DHP [61] and 2D keypoints from\n",
      "image datasets such as COCO [55], MPII [4] and LSP [39].\n",
      "4.1.1 MoCap Yoga (MoYo) Dataset\n",
      "We capture a trained Yoga professional in 200 highly com-\n",
      "plex poses (see Fig. 4) using a synchronized MoCap system,\n",
      "pressure mat, and a multi-view RGB video system with\n",
      "8 static, calibrated cameras; for details see Sup. Mat. The\n",
      "dataset contains ∼1.75M RGB frames in 4K resolution with\n",
      "ground-truth SMPL-X [68], pressure and CoM . Compared\n",
      "to the Fit3D [20] and PosePrior [1] datasets, MoYo is more\n",
      "challenging; it has extreme poses, strong self-occlusion, and\n",
      "significant body-ground and self-contact.\n",
      "4.2. Evaluation Metrics\n",
      "We use standard 3D HPS metrics: The Mean Per-Joint\n",
      "Position Error ( MPJPE ), its Procrustes Aligned version\n",
      "(PA-MPJPE), and the Per-Vertex Error (PVE) [67].\n",
      "BoS Error (BoSE). To evaluate stability, we propose a new\n",
      "metric called BoS Error ( BoSE ). Following the definition of\n",
      "stability (Sec. 3.3) we define:\n",
      "BoSE =(\n",
      "1g(¯m)∈ C(C)\n",
      "0g(¯m)/∈ C(C)(10)where C(C)is the convex hull of the gravity-projected con-\n",
      "tact vertices for τ= 10 cm. For efficiency reasons, we\n",
      "formulate this computation as the solution of a convex sys-\n",
      "tem via interior point linear programming [3]; see Sup. Mat.\n",
      "4.3. IPMAN Evaluation\n",
      "IPMAN-R. We evaluate our regressor, IPMAN-R , on\n",
      "RICH andH3.6M and summarize our results in Tab. 1. We\n",
      "refer to our regression baseline as HMR∗which is HMR\n",
      "trained on the same datasets as IPMAN-R . Since we train\n",
      "with paired 3D datasets, we do not use HMR ’s discriminator\n",
      "during training. Both IP terms individually improve upon\n",
      "the baseline method. Their joint use, however, shows the\n",
      "largest improvement. For example, on RICH theMPJPE\n",
      "improves by 3.5mm and the PVE by 2.5mm. It is particu-\n",
      "larly interesting that IPMAN-R improves upon the baseline\n",
      "onH3.6M , a dataset with largely dynamic poses and lit-\n",
      "tle body-ground contact. We also significantly outperform\n",
      "(∼12%) the MPJPE of optimization approaches that use\n",
      "the ground plane, Zou et al. [118] (69.9 mm) and Zanfir et\n",
      "al. [106] (69.0 mm), on H3.6M . Some video-based methods\n",
      "[51, 104] achieve better MPJPE (56.7and52.5resp.) on\n",
      "H3.6M . However, they initialize with a stronger kinematic\n",
      "predictor [46, 52] and require video frames as input. Further,\n",
      "they use heuristics to estimate body weight and non-physical\n",
      "residual forces to correct for contact estimation errors. In\n",
      "contrast, IPMAN is a single-frame method, models complex\n",
      "full-body pressure and does not rely on approximate body\n",
      "weight to compute CoM . Qualitatively, Fig. 5 (top) shows\n",
      "thatIPMAN-R ’s reconstructions are more stable and contain\n",
      "physically-plausible body-ground contact. While HMR is\n",
      "not SOTA, it is simple, isolating the benefits of our new IP\n",
      "formulation. These terms can also be added to methods with\n",
      "more modern backbones and architectures.\n",
      "IPMAN-O. Our optimization method, IPMAN-O ,\n",
      "also improves upon the baseline optimization method,\n",
      "SMPLify-XMC , on all evaluation metrics (see Tab. 2). We\n",
      "note that adding Lstability independently improves the PVE,\n",
      "but not joint metrics ( PA-MPJPE ,MPJPE ) and BoSE . This\n",
      "can be explained by the dependence of our IP terms on the\n",
      "relative position of the mesh surface to the ground-plane.\n",
      "Since joint metrics do not capture surfaces, they may get\n",
      "worse. Similar trends on joint metrics have been reported in\n",
      "the context of hand-object contact [29, 84] and body-scene\n",
      "contact [27]. We show qualitative results in Fig. 5 (bottom).\n",
      "While both SMPLify-XMC [64] and IPMAN-O achieve sim-\n",
      "ilar image projections, another view reveals that our results\n",
      "are more stable and physically plausible w.r.t. the ground.\n",
      "4.4. Pressure, CoP and CoM Evaluation\n",
      "We evaluate our estimated pressure, CoP and CoM\n",
      "against the MoYo ground truth. For pressure evaluation,\n",
      "we measure Intersection-over-Union (IoU) between our esti-\n",
      "6\n",
      "Figure 4. Representative examples illustrating the variation and complexity of 3D pose and body-ground contact in our new MoYo dataset.\n",
      "HMR (baseline) \n",
      "Camera View Side View \n",
      "IPMAN-R (ours) \n",
      "Camera View Side View Input Image \n",
      "Val 399 \n",
      " Val 693 \n",
      "Test 396 Pressure Map Pressure Map \n",
      "RICH \n",
      "Smplify-XMC (baseline) IPMAN-O (ours) MoYo \n",
      "Figure 5. Qualitative evaluation of IPMAN-R andIPMAN-O on the RICH andMoYo datasets. The first column shows the input images of\n",
      "a subject doing various sports poses. The second and third block of columns show the baseline’s and our results, respectively. In each block,\n",
      "the first image shows the estimated mesh overlayed on the image (camera view), the second image shows the estimated mesh in the world\n",
      "frame (side view), and the last image shows the estimated pressure map with the CoM (in pink) and the CoP (in green).\n",
      "7\n",
      "MethodRICH Human3.6M\n",
      "MPJPE ↓PAMPJPE ↓ PVE↓ BoSE (%) ↑MPJPE ↓PAMPJPE ↓\n",
      "PhysCap [79] - - - - 113.0 68.9\n",
      "DiffPhy [21] - - - - 81.7 55.6\n",
      "Zou et al. [118] - - - - 69.9 -\n",
      "Xie et al. [96] - - - - 68.1 -\n",
      "VIBE [46] - - - - 61.3 43.1\n",
      "Simpoe [104] - - - - 56.7 41.6\n",
      "D&D [51] - - - - 52.5 35.5\n",
      "HMR [43] - - - - 88.0 56.8\n",
      "Zanfir et al. [106] - - - - 69.0 -\n",
      "SPIN [49] 112.2 71.5 129.5 54.7 62.3 41.9\n",
      "PARE [47] 107.0 73.1 125.0 74.4 - -\n",
      "CLIFF [53] 107.0 67.2 122.3 67.6 81.4 52.1\n",
      "Finetuning on Human3.6M\n",
      "HMR∗[43] - - - - 62.1 41.6\n",
      "IPMAN-R (Ours) - - - - 60.7 (-1.4) 41.1 (-0.5)\n",
      "Finetuning on all datasets\n",
      "HMR∗[43] 82.5 48.3 92.4 62.0 61.6 41.9\n",
      "HMR∗[43]+Lground 80.9 47.8 89.9 66.5 61.9 41.8\n",
      "HMR∗[43]+Lstability 81.0 47.5 (-0.8) 90.8 69.6 61.2 41.9\n",
      "IPMAN-R (Ours) 79.0 (-3.5) 47.6 89.9 (-2.5) 71.2 (+9.2) 60.6 (-1.0) 41.8 (-0.1)\n",
      "Table 1. Top to Bottom: Comparisons with video-based and single-\n",
      "frame regression methods. IPMAN-R outperforms the single-frame\n",
      "baselines across all benchmarks. * indicates training hyperparam-\n",
      "eters and datasets are identical to IPMAN-R . All units are in mm\n",
      "except BoSE . Bold denotes best results (per category), and paren-\n",
      "theses show improvement over the baseline. /searcZoom in\n",
      "###### selected pressure map comparions \n",
      "220923_yogi_body_hands_03596_Bound_Angle_Pose_or_Baddha_Konasana_-b_930_gt.png \n",
      "220923_yogi_body_hands_03596_Bound_Angle_Pose_or_Baddha_Konasana_-b_930_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Bound_Angle_Pose_or_Baddha_Konasana_-e_876_gt.png \n",
      "220923_yogi_body_hands_03596_Bound_Angle_Pose_or_Baddha_Konasana_-e_876_pred_iou_first.png \n",
      "220923_yogi_body_hands_03596_Bridge_Pose_or_Setu_Bandha_Sarvangasana_-b_1288_gt.png \n",
      "220923_yogi_body_hands_03596_Bridge_Pose_or_Setu_Bandha_Sarvangasana_-b_1288_pred_iou_first.png \n",
      "220923_yogi_body_hands_03596_Cat_Cow_Pose_or_Marjaryasana_-a_478_gt.png \n",
      "220923_yogi_body_hands_03596_Cat_Cow_Pose_or_Marjaryasana_-a_478_pred_iou_first.png \n",
      "220923_yogi_body_hands_03596_Child_Pose_or_Balasana_-a_852_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Child_Pose_or_Balasana_-a_852_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Cow_Face_Pose_or_Gomukhasana_-b_962_gt.png \n",
      "220923_yogi_body_hands_03596_Cow_Face_Pose_or_Gomukhasana_-b_962_pred_iou_first.png \n",
      "220923_yogi_body_hands_03596_Extended_Puppy_Pose_or_Uttana_Shishosana_-a_1162_gt.png \n",
      "220923_yogi_body_hands_03596_Extended_Puppy_Pose_or_Uttana_Shishosana_-a_1162_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Four-Limbed_Staff_Pose_or_Chaturanga_Dandasana_-b_280_gt.png \n",
      "220923_yogi_body_hands_03596_Four-Limbed_Staff_Pose_or_Chaturanga_Dandasana_-b_280_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Garland_Pose_or_Malasana_-a_272_gt.png \n",
      "220923_yogi_body_hands_03596_Garland_Pose_or_Malasana_-a_272_pred_iou_first.png \n",
      "220923_yogi_body_hands_03596_Half_Moon_Pose_or_Ardha_Chandrasana_-a_696_gt.png \n",
      "220923_yogi_body_hands_03596_Half_Moon_Pose_or_Ardha_Chandrasana_-a_696_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Low_Lunge_pose_or_Anjaneyasana_-d_760_gt.png \n",
      "220923_yogi_body_hands_03596_Low_Lunge_pose_or_Anjaneyasana_-d_760_pred_cop_error_first.png \n",
      "220923_yogi_body_hands_03596_Scorpion_pose_or_vrischikasana-c_1306_gt.png \n",
      "220923_yogi_body_hands_03596_Scorpion_pose_or_vrischikasana-c_1306_pred_cop_error_first.png \n",
      "Figure 6. Qualitative comparison of estimated vs the ground-truth\n",
      "pressure. The ground-truth CoP is shown in green and the estimated\n",
      "CoP is shown in yellow. Pressure heatmap colors as per Fig. 2.\n",
      "mated and ground-truth pressure heatmaps. We also compute\n",
      "theCoP error as the Euclidean distance between estimated\n",
      "and ground-truth CoP. We obtain an IoU of 0.32and a CoP\n",
      "error of 57.3mm. Figure 6 shows a qualitative visualization\n",
      "of the estimated pressure compared to the ground truth. For\n",
      "CoM evaluation, we find a 53.3mm difference between our\n",
      "pCoM and the CoM computed by the commercial software,\n",
      "Vicon Plug-in Gait. Unlike Vicon’s estimate, our pCoM\n",
      "does not require anthropometric measurements and takes\n",
      "into account the full 3D body shape. For details about the\n",
      "evaluation protocol and comparisons with alternative CoM\n",
      "formulations, see Sup. Mat.\n",
      "Physics Simulation. To evaluate stability, we run a post-\n",
      "hoc physics simulation in “Bullet” [10] and measure the\n",
      "displacement of the estimated meshes; a small displacement\n",
      "denotes a stable pose. IPMAN-O produces 14.8%more\n",
      "stable bodies than the baseline [64]; for details see Sup. Mat.MethodMoYo\n",
      "MPJPE ↓PAMPJPE ↓ PVE↓ BoSE (%) ↑\n",
      "SMPLify-XMC [64] 75.3 36.5 16.8 98.0\n",
      "SMPLify-XMC [64] +Lground 73.3 36.2 14.5 98.2\n",
      "SMPLify-XMC [64] +Lstability 88.5 38.6 15.3 97.8\n",
      "IPMAN-O (Ours) 71.9 (-3.4) 34.3 (-2.2) 11.4 (-5.4) 98.6 (+0.5)\n",
      "Table 2. Evaluation of IPMAN-O and SMPLify-XMC [64]\n",
      "(optimization-based) on MoYo . Bold shows the best performance,\n",
      "and parentheses show the improvement over SMPLify-XMC.\n",
      "5. Conclusion\n",
      "Existing 3D HPS estimation methods recover SMPL\n",
      "meshes that align well with the input image, but are often\n",
      "physically implausible. To address this, we propose IPMAN ,\n",
      "which incorporates intuitive-physics in 3D HPS estimation.\n",
      "Our IP terms encourage stable poses, promote realistic floor\n",
      "support, and reduce body-floor penetration. The IP terms ex-\n",
      "ploit the interaction between the body CoM ,CoP, and BoS –\n",
      "key elements used in stability analysis. To calculate the\n",
      "CoM of SMPL meshes, IPMAN uses on a novel formulation\n",
      "that takes part-specific mass contributions into account. Ad-\n",
      "ditionally, IPMAN estimates proxy pressure maps directly\n",
      "from images, which is useful in computing CoP.IPMAN is\n",
      "simple, differentiable, and compatible with both regression\n",
      "and optimization methods. IPMAN goes beyond previous\n",
      "physics-based methods to reason about arbitrary full-body\n",
      "contact with the ground. We show that IPMAN improves\n",
      "both regression and optimization baselines across all metrics\n",
      "on existing datasets and MoYo. MoYo uniquely comprises\n",
      "synchronized multi-view video, SMPL-X bodies in com-\n",
      "plex poses, and measurements for pressure maps and body\n",
      "CoM . Qualitative results show the effectiveness of IPMAN\n",
      "in recovering physically plausible meshes.\n",
      "While IPMAN addresses body-floor contact, future work\n",
      "should incorporate general body-scene contact and diverse\n",
      "supporting surfaces by integrating 3D scene reconstruction.\n",
      "In this work, the proposed IP terms are designed to help\n",
      "static poses and we show that they do not hurt dynamic\n",
      "poses. However, the large body of biomechanical literature\n",
      "analyzing dynamic poses could be leveraged for activities\n",
      "like walking, jogging, running, etc. It would be interesting to\n",
      "extend IPMAN beyond single-person scenarios by exploiting\n",
      "the various physical constraints offered by multiple subjects.\n",
      "Acknowledgements. We thank T. Alexiadis, T. McConnell, C. Gal-\n",
      "latz, M. H ¨oschle, S. Polikovsky, C. Mendoza, Y . Fincan, L. Sanchez\n",
      "and M. Safroshkin for data collection, G. Becherini for MoSh++,\n",
      "N. Athanasiou, Z. Fang, V . Choutas and all of Perceiving Systems\n",
      "for fruitful discussions. This work was funded by the International\n",
      "Max Planck Research School for Intelligent Systems (IMPRS-IS)\n",
      "and in part by the German Federal Ministry of Education and Re-\n",
      "search (BMBF), T ¨ubingen AI Center, FKZ: 01IS18039B.\n",
      "Disclosure. https://files.is.tue.mpg.de/black/CoI CVPR 2023.txt\n",
      "8\n",
      "Supplementary Material\n",
      "A. MoCap Yoga Dataset (MoYo)\n",
      "We capture a trained yoga professional in a MoCap stu-\n",
      "dio with 54 Vicon Vantage V16 infrared cameras capable of\n",
      "tracking body markers as small as 3mm in diameter. The Vi-\n",
      "con system was synchronized with 8 RGB cameras recording\n",
      "at 4112x3008 resolution and a Zebris FDM pressure mea-\n",
      "surement mat. The pressure mat offers a sensor resolution\n",
      "of 1.4sensors/cm2and can capture pressure in 10-1200 kPa\n",
      "range. Ground-truth SMPL-X [68] parameters are recovered\n",
      "from the MoCap data using MoSh++ [59]. A total of 200\n",
      "yoga sequences were recorded at 30fps. The yoga poses we\n",
      "selected include all poses in the Yoga-82 dataset [86] as well\n",
      "as their variations. The T-SNE [85] plot in Fig. S.1 shows\n",
      "that the poses contained in MoYo are highly diverse and\n",
      "cover areas in the space of human poses not well represented\n",
      "in existing datasets [37, 59, 61, 67].\n",
      "To compute a reference CoM , we use the commercially\n",
      "available tool, Plug-in Gait (PiG) from Vicon. PiG requires\n",
      "a-priori known anthropometric measurements ( e.g.height,\n",
      "weight, shoulder offset, knee width, etc) and computes: (1)\n",
      "bone joints from a known marker topology, (2) per-bone\n",
      "mass as a proportion of body mass, (3) per-bone CoM as a\n",
      "proportion of each bone’s length, and (4) whole-body CoM\n",
      "as a weighted average of per-bone CoM s. In contrast, our\n",
      "pCoM does not require anthropometric measurements and\n",
      "takes into account the full 3D body shape.\n",
      "Figure S.1. The distribution of poses in MoYo and existing MoCap\n",
      "datasets are visualized after T-SNE dimension reduction.\n",
      "Figure S.2. Distribution of Lstability in AMASS and MoYo. Both\n",
      "peak at ∼0, motivating using an L2formulation. Bottom right:\n",
      "Unstable long-tail poses from AMASS.\n",
      "B. Method\n",
      "B.1. Stability Loss\n",
      "The suggested classic definition uses a binary stability\n",
      "criterion, i.e., theCoM “just” projects either inside or outside\n",
      "the BoS. This is discontinuous with sparse gradients.\n",
      "Since CoP lies inside BoS, our L2 loss is a “soft” version\n",
      "that approximates the classic definition, but has two key\n",
      "benefits: (1) it is continuous and fully differentiable, and, (2)\n",
      "it informs about the degree of instability. The distribution\n",
      "ofLstability in Fig. S.2 for both AMASS and MoYo datasets\n",
      "peak at ∼0, motivating using an L2formulation.\n",
      "B.2. Elements of Stability Analysis: Alternative for-\n",
      "mulations\n",
      "Computation of the “Center of Mass”, CoM , must be\n",
      "efficient and differentiable. The CoM could be naively ap-\n",
      "proximated as the mean vertex position of a mesh:\n",
      "¯mnaive=1\n",
      "NVNVX\n",
      "i=1vi. (S.1)\n",
      "However, the SMPL and the SMPL-X body models have a\n",
      "non-uniform vertex distribution across the surface. There\n",
      "are a disproportionate number of vertices on the face and\n",
      "hands compared to the body. For instance, roughly half of\n",
      "SMPL-X ’s vertices lie on the head. Consequently, ¯mnaiveis\n",
      "dominated by face and hand vertices.\n",
      "A better formulation is the mean of uniformly sampled\n",
      "surface points:\n",
      "¯mu\n",
      "naive=1\n",
      "NUNUX\n",
      "i=1vi. (S.2)\n",
      "9\n",
      "Pose COM-naive COM-trig COM-part COS-naive COS-pressure \n",
      "Pose \n",
      "Figure S.3. Gravity-projections of different formulations of CoM (shown with pink) and CoP (shown with green) are shown with estimated\n",
      "pressure maps. Our proposed pCoM captures more accurate body mass distribution because it takes into account part-specific mass\n",
      "contributions. Similarly, our CoP leverages the pressure maps rather than binary contact.\n",
      "Another formulation computes the average of the mesh\n",
      "triangle face centroids weighted by the face area:\n",
      "¯mtrig=PNF\n",
      "i=1Ai¯FiPNF\n",
      "i=1Ai, (S.3)\n",
      "where Aidenotes the area and ¯Fi=1\n",
      "3(v⊤\n",
      "i1+v⊤\n",
      "i2+v⊤\n",
      "i3)the\n",
      "centroid of face Fi. The problem with these approaches is\n",
      "that they assume that mass, M, is proportional to surface\n",
      "area,S, which is a poor approximation.\n",
      "Our proposed pCoM formulation addresses this by (1)\n",
      "uniformly sampling vertices on the SMPL mesh and (2)\n",
      "taking part-specific mass contributions into account. Our\n",
      "pCoM computes mass from volume, V, via the standard\n",
      "density equation, M=ρV. Tab. S.1 compares the CoM\n",
      "error across different formulations of CoM w.r.t. ground-\n",
      "truth CoM obtained using Vicon PiG. pCoM significantly\n",
      "outperforms all baselines. Figure S.3 shows an intuitive\n",
      "qualitative comparison between all formulations of CoM.\n",
      "¯mnaive ¯mu\n",
      "naive ¯mtrig pCoM ( ¯m)\n",
      "CoM error ↓264.1 mm 68.5 mm 70.0 mm 53.3 mm\n",
      "Table S.1. Comparison of various CoM formulations.\n",
      "Similarly, for “Center of Pressure” ( CoP), a simple\n",
      "heuristic used in previous works detects binary contact by\n",
      "thresholding body vertices using their Euclidean distance\n",
      "from the ground plane. However, such contact lacks in-\n",
      "formation about the pressure distribution and assigns equal\n",
      "weight to all contact vertices. Moreover, binary contactis not differentiable and is therefore generally used at test-\n",
      "time [27,87,106,112] or for data preprocessing [28,113], not\n",
      "during training. In contrast, our CoP formulation is fully dif-\n",
      "ferentiable and takes the inferred pressure distribution of the\n",
      "body-floor contact into account. As shown in Fig. S.3, the\n",
      "naive CoP suffers from equally weighting all binary-contact\n",
      "whereas our CoP better represents the pressure profile of the\n",
      "body-ground contact.\n",
      "B.3. Ablation of ground losses\n",
      "Figure S.4. Left: The gradient of Lpulldecays gradually with\n",
      "h(pi); vertices with h(pi)≥20cm contribute minimally to back-\n",
      "propagation. Right : Effect of LpushandLpull; highLpushresults in\n",
      "inaccurate floor contact, high Lpullresults in penetrations.\n",
      "Instead of having a threshold to restrict Lpullonly to ver-\n",
      "tices close to the ground, we chose a soft version of the\n",
      "loss to ensure full differentiability. However, as shown in\n",
      "Fig. S.4 (left), the loss gradient decays with height and ver-\n",
      "tices with h(vi)≥15cm contribute minimally during back-\n",
      "propagation. Further, we study the impact of LpushandLpull\n",
      "in Tab. S.2 and Fig. S.4-right. The terms complement each\n",
      "other and are more effective when used jointly ( Lground ).\n",
      "10\n",
      "Method MPJPE ↓PMPJPE ↓PVE↓\n",
      "HMR∗[43] 82.5 48.2 92.3\n",
      "HMR∗[43]+Lpush 85.4 49.0 96.6\n",
      "HMR∗[43]+Lpull 88.0 48.8 99.4\n",
      "HMR∗[43]+Lground 80.9 47.8 89.9\n",
      "Table S.2. Ablation for LpushandLpullon the RICH [35] dataset.\n",
      "C. Experiments\n",
      "We integrate our intuitive-physics terms in both an\n",
      "optimization- and a regression-based method for three rea-\n",
      "sons: (1) the community heavily uses both method types,\n",
      "(2) our terms generalize and benefit both types, despite their\n",
      "differences, and (3) our terms also work with different body\n",
      "models; SMPL-X (used by IPMAN-O ) and SMPL (used by\n",
      "IPMAN-R).\n",
      "C.1. IPMAN Implementation Details\n",
      "C.1.1 IPMAN-R.\n",
      "Similar to previous methods [38, 48, 49, 64], we take the\n",
      "widely used HMR [43] architecture to analyze the effect of\n",
      "adding our proposed IP terms. Note that, while HMR is not\n",
      "the most recent method, it is widely used as a backbone.\n",
      "As such, it provides a consistent foundation for evaluation\n",
      "and comparison. Our goal here is to isolate and evaluate the\n",
      "effect of adding intuitive physics. Such terms should then be\n",
      "readily applicable to other HPS regression frameworks.\n",
      "The HMR regressor estimates the camera translation\n",
      "tcand SMPL parameters (pose, global orientation, and\n",
      "shape) in the camera coordinates assuming Rc=I3and\n",
      "tb=0. We initialize the HMR model using pretrained\n",
      "weights provided by SPIN [49] and finetune both IPMAN-R\n",
      "andHMR on the same datasets; namely RICH [35], Hu-\n",
      "man3.6M [37], MPI-INF-3DHP [61], COCO [55], MPII [4]\n",
      "and LSP [39, 40]. In the main paper, we call the baseline\n",
      "asHMR∗which uses the same training datasets and hyper-\n",
      "parameters as IPMAN-R , albeit with the exception of the\n",
      "proposed IP terms. We follow the same training schedule,\n",
      "data augmentation and hyperparameters as SPIN [49] but\n",
      "do not use in-the-loop optimization. We use the Adam op-\n",
      "timizer with learning rate of 5e−5and finetuning takes 3\n",
      "epochs ( ∼8hours) on a Nvidia Tesla V100 GPU.\n",
      "We set the hyperparameters α= 100 ,γ= 10 for the\n",
      "per-vertex pressure ρi,α1= 1.0,α2= 0.15for the Lpull\n",
      "term and β1= 10 .0,β2= 0.15for the Lpushterm. The\n",
      "loss weights are empirically determined to be λs= 0.01and\n",
      "λg= 0.01. We borrow the same configuration as [49] for all\n",
      "remaining loss weights, namely λ2D,λ3DandλSMPL .\n",
      "RICH [35] contains sequences with an uneven ground-\n",
      "plane. For training IPMAN-R , we therefore sample a subset\n",
      "of the RICH dataset where subjects mainly interact with aneven ground plane (see Tab. S.3). In the Train/Val sequences,\n",
      "we use camera 0 for validation and cameras 1-5 for training.\n",
      "Train/Val Test\n",
      "’Pavallion 000yoga2’ ‘Pavallion 002yoga1’\n",
      "’Pavallion 000yoga1’ ‘Pavallion 013yoga2’\n",
      "’Pavallion 006yoga1’ ‘ParkingLot2 014pushup2’\n",
      "’Pavallion 018yoga1’ ‘ParkingLot1 005pushup1’\n",
      "Table S.3. Training, validation and test sequences in the RICH\n",
      "dataset containing an even ground.\n",
      "C.1.2 IPMAN-O.\n",
      "ForIPMAN-O , we extend the baseline optimization-based\n",
      "method SMPLify-XMC [64]. We use the same configuration\n",
      "asSMPLify-XMC and only add extra hyperparameters for\n",
      "the proposed IP terms. Both methods are initialized with the\n",
      "same presented pose from the MoYo dataset. We extract 2D\n",
      "keypoints from images using MediaPipe [57].\n",
      "Same as IPMAN-R , we set the hyperparameters α= 70 ,\n",
      "γ= 10 for the per-vertex pressure ρi,α1= 1.0,α2= 0.15\n",
      "for the Lpullterm and β1= 10 .0,β2= 0.15forLpush\n",
      "term. The loss weights are empirically determined to be\n",
      "λs= 10000 andλg= 10000 .\n",
      "C.2. Evaluation Metrics\n",
      "C.2.1 BoS Error (BoSE) calculation.\n",
      "Recall that the “Base of Support” ( BoS) is defined by the\n",
      "convex hull of the contact regions. Since computing this\n",
      "can be computationally inefficient, we reformulate the BoSE\n",
      "computation to test if projection of the CoM ,g(¯mpart), on\n",
      "the ground plane can be represented as a convex combi-\n",
      "nation of the gravity-projected contact vertices C. To this\n",
      "end, we solve the linear equation system via standard linear\n",
      "programming using interior point methods [3]:\n",
      "min a∥a⊤C−¯mpart∥ (S.4)\n",
      "s.t. ai∈a≥0 (S.5)X\n",
      "ai= 1 (S.6)\n",
      "where a⊤C=a1c1+···+ancnfor the points ciinC. If\n",
      "the system has a solution, g(¯m)∈ C(C)holds, otherwise\n",
      "g(¯m)is not in the convex hull of C, i.e.g(¯m)/∈ C(C).\n",
      "C.3. Qualitative Results\n",
      "Figures S.6 and S.7 show supplemental qualitative results\n",
      "for IPMAN-R and IPMAN-O, respectively.\n",
      "11\n",
      "Figure S.5. Stability evaluation using the “Bullet” physics engine.\n",
      "Meshes produced by the baseline method [64] (in orange) top-\n",
      "ple but IPMAN-O ’s meshes (in cyan) remain stable after physics\n",
      "simulation.\n",
      "D. Stability Evaluation via Physics Simulation\n",
      "Current physics engines are incompatible with HPS\n",
      "methods, as they approximate SMPL bodies with rigid\n",
      "convex hulls and are non-differentiable. However, us-\n",
      "ing them for posthoc stability evaluation of the estimated\n",
      "meshes is possible. Specifically, we evaluate IPMAN-O\n",
      "andSMPLify-XMC [64] by first, using V-HACD convex\n",
      "decomposition [60] of the estimated body meshes and then\n",
      "by simulating physics as in [29, 84] via the “Bullet” physics\n",
      "engine [10]. We measure the displacement of the human\n",
      "mesh after 100 physics simulation steps; a small displace-\n",
      "ment denotes a stable pose and vice versa. IPMAN-O pro-\n",
      "duces 14.8%more stable bodies than the baseline [64]; see\n",
      "Fig. S.5.\n",
      "E. Evaluation of Biomechanical Elements\n",
      "We use the pressure field defined in Eqn. 2 of the main\n",
      "paper to compute per-point pressure on the SMPL mesh.\n",
      "With this, the pressure heatmap is estimated by summing the\n",
      "per-point pressure projected to the ground-plane. Note that\n",
      "we recover relative pressure as we do not assume availability\n",
      "of ground-truth body mass or anthropometric measurements.\n",
      "To measure the overlap of the inferred pressure heatmap\n",
      "w.r.t. the ground-truth, we compute the intersection-over-\n",
      "union (IOU) between the two. However, the ZEBRIS pres-\n",
      "sure sensor captures pressure measurements in the range\n",
      "10-1200 KPa. Depending upon the contact area and the\n",
      "weight of the subject, some poses may fall outside this range.\n",
      "For instance, a person lying-down only exerts 1-5 kPa of\n",
      "pressure on the ground. To account for this, we tune the\n",
      "sensitivity of our pressure field for every pose and report\n",
      "mean of the best per-sample IOU.\n",
      "We measure accuracy of our CoP by simply computing\n",
      "the Euclidean distance w.r.t. ground-truth. We call this as\n",
      "CoP error. Again, we report mean of the best CoP error after\n",
      "tuning the sensitivity of our inferred pressure field.TheCoM error is similar to the CoP error, albiet in 3D.\n",
      "It measures the Euclidean distance between the estimated\n",
      "and ground-truth CoM recovered from Vicon Plug-in Gait.\n",
      "Table S.4 presents summary results showing that our inferred\n",
      "pressure, CoP and CoM agrees with the ground-truth.\n",
      "Pressure CoM\n",
      "mIOU CoP error (mm) CoM error (mm)\n",
      "IPMAN (Ours) 0.32 57.3 53.3\n",
      "Table S.4. Quantitative evaluations of our estimated pressure, CoP\n",
      "and CoM w.r.t. ground-truth in MoYo.\n",
      "F. IPMAN-O* (Extension of SMPLify-X).\n",
      "To further explore the effect of our intuitive-physics terms,\n",
      "we extend the optimization method SMPLify-X [68] and\n",
      "name this IPMAN-O* (note that this is different from the\n",
      "main paper’s IPMAN-O that extends SMPLify-XMC ). We\n",
      "fit the SMPL-X body model to 2D image keypoints starting\n",
      "from mean pose and shape while exploiting the ground-truth\n",
      "ground plane. Adapted from SMPLify-X [68], we minimize\n",
      "the objective\n",
      "E(β,θ,ψ,tc) =EJ2D+Eθ+λβEβ+λψEψ+\n",
      "λαEα+λCEC+ (S.7)\n",
      "λsEstability +λgEground .\n",
      "The energy term EJ2Ddenotes the 2D re-projection error\n",
      "whereas the remaining terms Eθ=λθbEθb+λθfEθf+\n",
      "λθhEθhrepresent various priors for body, face, and hand\n",
      "pose. Eβ,Eψ,EαandECare prior terms for body shape,\n",
      "expression, extreme bending and self-penetration (see [68]\n",
      "for details). ESandEGare the stability and ground contact\n",
      "losses. The results in Tab. S.5 show a clear improvement.\n",
      "Note that SMPLify-X estimates the body’s global orienta-\n",
      "tionRband the camera translation tc, while camera rotation\n",
      "Rcand body translation tbremain zero. In order to apply our\n",
      "IP terms, we use the ground-truth camera rotation Rc\n",
      "wand\n",
      "translation tc\n",
      "wto transform the estimated mesh from camera\n",
      "to world coordinates. We empirically find that applying the\n",
      "IP terms to the final stage of optimization in SMPLify-X\n",
      "gives more accurate results than applying them to all stages.\n",
      "We hypothesize that this could be due to having a better body\n",
      "initialization before applying the IP terms.\n",
      "G. Evaluation on 3DPW\n",
      "3DPW [88] is an outdoor dataset containing pseudo\n",
      "ground-truth SMPL and camera parameters recovered using\n",
      "IMU sensors attached to the actors. As also noted in [102],\n",
      "we find that the ground plane in 3DPW is inconsistent. In\n",
      "fact, two subjects in the same scene can be supported by\n",
      "12\n",
      "HMR (baseline) \n",
      "Camera Projection View 1 Pressure Map IPMAN-R (ours) \n",
      "Camera Projection View 1 Pressure Map Input Image \n",
      "Test \n",
      "216 \n",
      "Test\n",
      "  \n",
      "=377 \n",
      "  \n",
      "Figure S.6. Additional qualitative evaluation of IPMAN-R onRICH . The first column shows the input images of a subject doing various\n",
      "sports poses. The second and third block of columns show the results of HMR (baseline) and IPMAN-R , respectively. In each block, the\n",
      "first image shows the estimated mesh overlaid on the image. The next three images show different views of the estimated mesh in the world\n",
      "frame. The green sphere illustrates the CoM.\n",
      "MethodRICH [35]\n",
      "MPJPE ↓PVE↓BoSE (%) ↑\n",
      "SMPLify-X [68] 268.6 228.3 96.9\n",
      "IPMAN-O* (Ours) 240.9 217.1 98.0\n",
      "Table S.5. IPMAN-O* compared to the optimization method of\n",
      "[68] on RICH [35].\n",
      "Method3DPW [88]\n",
      "MPJPE ↓PMPJPE ↓\n",
      "SPIN [49] 97.2 59.6\n",
      "IPMAN-R (Ours) 96.8 57.1\n",
      "Table S.6. IPMAN-R compared to the regression method of [49]\n",
      "on 3DPW [88].\n",
      "different ground-planes in the world coordinates. Addition-\n",
      "ally, 3DPW primarily contains dynamic poses like walking,\n",
      "climbing stairs, parkour, etc. Due to these reasons, 3DPW\n",
      "does not satisfy the core assumptions of IPMAN . Neverthe-\n",
      "less, we report results on 3DPW to show that the IP terms\n",
      "do not degrade performance for such datasets; in fact, wesee a slight improvement in performance as illustrated in Ta-\n",
      "ble S.6. This makes IPMAN applicable to everyday motion\n",
      "without needing special care.\n",
      "13\n",
      "Camera \n",
      "Projection View 1 Pressure \n",
      "Map\n",
      "Camera Projection View 1 Underside view Input Image \n",
      "View 2 Camera \n",
      "Projection View 1 Pressure \n",
      "MapSimplify-XMC IPMAN-O (ours) \n",
      "s\n",
      "t\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "p\n",
      "e\n",
      "n\n",
      "e\n",
      "t\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "F\n",
      "L\n",
      "Y\n",
      "I\n",
      "N\n",
      "G\n",
      "Figure S.7. Qualitative evaluation of IPMAN-O onMoYo . In the first column, the input images of a subject doing yoga poses. The second\n",
      "and third blocks show the results of the SMPLify-XMC andIPMAN-O respectively. In each block, the first and second column show the\n",
      "estimated mesh projected into the image and from a second view. The last images show the pressure map with the CoM (in pink) and the\n",
      "CoP (in green).\n",
      "References\n",
      "[1]Ijaz Akhter and Michael J. Black. Pose-conditioned joint\n",
      "angle limits for 3D human pose reconstruction. In Computer\n",
      "Vision and Pattern Recognition (CVPR) , pages 1446–1455,\n",
      "2015. 3, 6\n",
      "[2]Rıza Alp G ¨uler, Natalia Neverova, and Iasonas Kokkinos.\n",
      "DensePose: Dense human pose estimation in the wild. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "7297–7306, 2018. 3\n",
      "[3]Erling D. Andersen and Knud D. Andersen. The Mosek inte-\n",
      "rior point optimizer for linear programming: An implemen-\n",
      "tation of the homogeneous algorithm. In High PerformanceOptimization , 2000. 6, 11\n",
      "[4]Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\n",
      "Bernt Schiele. 2D human pose estimation: New benchmark\n",
      "and state of the art analysis. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 3686–3693, 2014. 5, 6, 11\n",
      "[5]Dragomir Anguelov, Praveen Srinivasan, Daphne Koller,\n",
      "Sebastian Thrun, Jim Rodgers, and James Davis. SCAPE:\n",
      "Shape completion and animation of people. Transactions on\n",
      "Graphics (TOG) , 24:408–416, 2005. 3\n",
      "[6]Michael Barnett-Cowan, Roland W. Fleming, Manish Singh,\n",
      "and Heinrich H. B ¨ulthoff. Perceived object stability depends\n",
      "on multisensory estimates of gravity. PLOS ONE , 6(4):1–5,\n",
      "14\n",
      "2011. 2\n",
      "[7]Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\n",
      "Gehler, Javier Romero, and Michael J. Black. Keep it SMPL:\n",
      "Automatic estimation of 3D human pose and shape from a\n",
      "single image. In European Conference on Computer Vision\n",
      "(ECCV) , volume 9909, pages 561–578, 2016. 3\n",
      "[8]Marcus A. Brubaker, David J. Fleet, and Aaron Hertzmann.\n",
      "Physics-based person tracking using the anthropomorphic\n",
      "walker. International Journal of Computer Vision (IJCV) ,\n",
      "87(1–2):140–155, 2010. 3\n",
      "[9]Marcus A. Brubaker, Leonid Sigal, and David J. Fleet. Esti-\n",
      "mating contact dynamics. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 2389–2396, 2009. 3\n",
      "[10] Bullet real-time physics simulation. https : / /\n",
      "pybullet.org . 1, 8, 12\n",
      "[11] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and\n",
      "Yaser Sheikh. OpenPose: Realtime multi-person 2D pose\n",
      "estimation using part affinity fields. Transactions on Pattern\n",
      "Analysis and Machine Intelligence (TPAMI) , 43(1):172–186,\n",
      "2021. 3\n",
      "[12] Yixin Chen, Sai Kumar Dwivedi, Michael J. Black, and Dim-\n",
      "itrios Tzionas. Detecting human-object contact in images.\n",
      "InComputer Vision and Pattern Recognition (CVPR) , June\n",
      "2023. 3\n",
      "[13] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-\n",
      "itrios Tzionas, and Michael J. Black. Monocular expressive\n",
      "body regression through body-driven attention. In European\n",
      "Conference on Computer Vision (ECCV) , volume 12355,\n",
      "pages 20–40, 2020. 3\n",
      "[14] Henry M. Clever, Zackory M. Erickson, Ariel Kapusta, Greg\n",
      "Turk, C. Karen Liu, and Charles C. Kemp. Bodies at rest:\n",
      "3D human pose and shape estimation from a pressure im-\n",
      "age using synthetic data. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 6214–6223, 2020. 3, 4\n",
      "[15] Enric Corona, Albert Pumarola, Guillem Aleny `a, Ger-\n",
      "ard Pons-Moll, and Francesc Moreno-Noguer. SMPLicit:\n",
      "Topology-aware generative model for clothed people. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "11875–11885, 2021. 3\n",
      "[16] Taosha Fan, Kalyan Vasudev Alwala, Donglai Xiang,\n",
      "Weipeng Xu, Todd Murphey, and Mustafa Mukadam. Revi-\n",
      "talizing optimization for 3D human pose and shape estima-\n",
      "tion: A sparse constrained formulation. In International Con-\n",
      "ference on Computer Vision (ICCV) , pages 11437–11446,\n",
      "2021. 3\n",
      "[17] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed\n",
      "Kocabas, Manuel Kaufmann, Michael J. Black, and Otmar\n",
      "Hilliges. ARCTIC: A dataset for dexterous bimanual hand-\n",
      "object manipulation. In Computer Vision and Pattern Recog-\n",
      "nition (CVPR) , June 2023. 3\n",
      "[18] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\n",
      "Tzionas, and Michael J. Black. Collaborative regression\n",
      "of expressive bodies using moderation. In International\n",
      "Conference on 3D Vision (3DV) , pages 792–804, 2021. 3\n",
      "[19] Mihai Fieraru, Mihai Zanfir, Teodor Alexandru Szente,\n",
      "Eduard Gabriel Bazavan, Vlad Olaru, and Cristian Smin-\n",
      "chisescu. REMIPS: Physically consistent 3D reconstruc-\n",
      "tion of multiple interacting people under weak supervision.InConference on Neural Information Processing Systems\n",
      "(NeurIPS) , volume 34, 2021. 3, 5\n",
      "[20] Mihai Fieraru, Mihai Zanfir, Silviu-Cristian Pirlea, Vlad\n",
      "Olaru, and Cristian Sminchisescu. AIFit: Automatic 3D\n",
      "human-interpretable feedback models for fitness training. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "9919–9928, 2021. 6\n",
      "[21] Erik G ¨artner, Mykhaylo Andriluka, Erwin Coumans, and\n",
      "Cristian Sminchisescu. Differentiable dynamics for articu-\n",
      "lated 3D human motion reconstruction. In Computer Vision\n",
      "and Pattern Recognition (CVPR) , pages 13180–13190, 2022.\n",
      "3, 8\n",
      "[22] Erik G ¨artner, Mykhaylo Andriluka, Hongyi Xu, and Cristian\n",
      "Sminchisescu. Trajectory optimization for physics-based\n",
      "reconstruction of 3D human pose from monocular video. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "13096–13105, 2022. 3\n",
      "[23] Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng\n",
      "Wang, and Liang Lin. Graphonomy: Universal human pars-\n",
      "ing via graph transfer learning. In Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 7450–7459, 2019. 3\n",
      "[24] Shanyan Guan, Jingwei Xu, Yunbo Wang, Bingbing Ni, and\n",
      "Xiaokang Yang. Bilevel online adaptation for out-of-domain\n",
      "human mesh reconstruction. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 10472–10481, 2021. 3\n",
      "[25] Riza Alp G ¨uler and Iasonas Kokkinos. HoloPose: Holistic\n",
      "3D human reconstruction in-the-wild. In Computer Vision\n",
      "and Pattern Recognition (CVPR) , pages 10876–10886, 2019.\n",
      "3\n",
      "[26] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-\n",
      "cent Lepetit. HOnnotate: A method for 3D annotation of\n",
      "hand and object poses. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 3193–3203, 2020. 5\n",
      "[27] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,\n",
      "and Michael J. Black. Resolving 3D human pose ambiguities\n",
      "with 3D scene constraints. In International Conference on\n",
      "Computer Vision (ICCV) , pages 2282–2292, 2019. 3, 6, 10\n",
      "[28] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios\n",
      "Tzionas, and Michael J. Black. Populating 3D scenes by\n",
      "learning human-scene interaction. In Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 14708–14718, 2021. 3,\n",
      "10\n",
      "[29] Yana Hasson, G ¨ul Varol, Dimitrios Tzionas, Igor Kale-\n",
      "vatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid.\n",
      "Learning joint reconstruction of hands and manipulated ob-\n",
      "jects. In Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 11807–11816, 2019. 5, 6, 12\n",
      "[30] Havok: Customizable, fully multithreaded, and highly op-\n",
      "timized physics simulation. http://www.havok.com .\n",
      "1\n",
      "[31] Eric Heiden, David Millard, Erwin Coumans, Yizhou Sheng,\n",
      "and Gaurav S. Sukhatme. NeuralSim: Augmenting differen-\n",
      "tiable simulators with neural networks. In International Con-\n",
      "ference on Robotics and Automation (ICRA) , pages 9474–\n",
      "9481, 2021. 3\n",
      "[32] At L. Hof. The equations of motion for a standing human re-\n",
      "veal three mechanisms for balance. Journal of Biomechanics ,\n",
      "40(2):451–457, 2007. 2, 4\n",
      "15\n",
      "[33] At L. Hof. The “extrapolated center of mass” concept sug-\n",
      "gests a simple control of balance in walking. Human move-\n",
      "ment science , 27(1):112–125, 2008. 2, 4\n",
      "[34] At L. Hof, M. G. J. Gazendam, and Sinke W. E. The condi-\n",
      "tion for dynamic stability. Journal of Biomechanics , 38(1):1–\n",
      "8, 2005. 4\n",
      "[35] Chun-Hao Huang, Hongwei Yi, Markus H ¨oschle, Matvey\n",
      "Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel\n",
      "Scharstein, and Michael Black. Capturing and inferring\n",
      "dense full-body human-scene contact. In Computer Vision\n",
      "and Pattern Recognition (CVPR) , pages 13264–13275, 2022.\n",
      "2, 3, 6, 11, 13\n",
      "[36] Leslie Ikemoto, Okan Arikan, and David Forsyth. Knowing\n",
      "when to put your foot down. In Symposium on Interactive\n",
      "3D Graphics (SI3D) , page 49–53, 2006. 3\n",
      "[37] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\n",
      "Sminchisescu. Human3.6M: Large scale datasets and predic-\n",
      "tive methods for 3D human sensing in natural environments.\n",
      "Transactions on Pattern Analysis and Machine Intelligence\n",
      "(TPAMI) , 36(7):1325–1339, 2014. 2, 5, 6, 9, 11\n",
      "[38] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei\n",
      "Zhou, and Kostas Daniilidis. Coherent reconstruction of\n",
      "multiple humans from a single image. In Computer Vision\n",
      "and Pattern Recognition (CVPR) , pages 5578–5587, 2020.\n",
      "3, 11\n",
      "[39] Sam Johnson and Mark Everingham. Clustered pose and\n",
      "nonlinear appearance models for human pose estimation. In\n",
      "British Machine Vision Conference (BMVC) , pages 1–11,\n",
      "2010. 5, 6, 11\n",
      "[40] Sam Johnson and Mark Everingham. Learning effective\n",
      "human pose estimation from inaccurate annotation. In Com-\n",
      "puter Vision and Pattern Recognition (CVPR) , pages 1465–\n",
      "1472, 2011. 11\n",
      "[41] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exem-\n",
      "plar fine-tuning for 3D human pose fitting towards in-the-\n",
      "wild 3D human pose estimation. In International Conference\n",
      "on 3D Vision (3DV) , pages 42–52, 2021. 3\n",
      "[42] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total capture:\n",
      "A 3D deformation model for tracking faces, hands, and\n",
      "bodies. In Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 8320–8329, 2018. 2, 3\n",
      "[43] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\n",
      "Jitendra Malik. End-to-end recovery of human shape and\n",
      "pose. In Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 7122–7131, 2018. 3, 5, 8, 11\n",
      "[44] Angjoo Kanazawa, Jason Y . Zhang, Panna Felsen, and Ji-\n",
      "tendra Malik. Learning 3D human dynamics from video.\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "5607–5616, 2019. 3\n",
      "[45] Rawal Khirodkar, Shashank Tripathi, and Kris Kitani. Oc-\n",
      "cluded human mesh recovery. In Computer Vision and Pat-\n",
      "tern Recognition (CVPR) , pages 1705–1715, 2022. 3\n",
      "[46] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\n",
      "Black. VIBE: Video inference for human body pose and\n",
      "shape estimation. In Computer Vision and Pattern Recogni-\n",
      "tion (CVPR) , pages 5252–5262, 2020. 3, 6, 8\n",
      "[47] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,\n",
      "and Michael J. Black. PARE: Part attention regressor for3D human body estimation. In International Conference on\n",
      "Computer Vision (ICCV) , pages 11127–11137, 2021. 1, 8\n",
      "[48] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,\n",
      "Lea M ¨uller, Otmar Hilliges, and Michael J. Black. SPEC:\n",
      "Seeing people in the wild with an estimated camera. In\n",
      "International Conference on Computer Vision (ICCV) , pages\n",
      "11035–11045, 2021. 11\n",
      "[49] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, and\n",
      "Kostas Daniilidis. Learning to reconstruct 3D human pose\n",
      "and shape via model-fitting in the loop. In International\n",
      "Conference on Computer Vision (ICCV) , pages 2252–2261,\n",
      "2019. 3, 5, 6, 8, 11, 13\n",
      "[50] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-\n",
      "ilidis. Convolutional mesh regression for single-image hu-\n",
      "man shape reconstruction. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 4496–4505, 2019. 3\n",
      "[51] Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, Gang Yu,\n",
      "and Cewu Lu. D&D: Learning human dynamics from dy-\n",
      "namic camera. In European Conference on Computer Vision\n",
      "(ECCV) , 2022. 3, 6, 8\n",
      "[52] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,\n",
      "and Cewu Lu. HybrIK: A hybrid analytical-neural inverse\n",
      "kinematics solution for 3D human pose and shape estimation.\n",
      "InComputer Vision and Pattern Recognition (CVPR) , pages\n",
      "3383–3393, 2021. 3, 6\n",
      "[53] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\n",
      "and Youliang Yan. CLIFF: Carrying location information in\n",
      "full frames into human pose and shape estimation. In ECCV ,\n",
      "volume 13665, pages 590–606, 2022. 1, 3, 8\n",
      "[54] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end\n",
      "human pose and mesh reconstruction with transformers. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "1954–1963, 2021. 3\n",
      "[55] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\n",
      "Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\n",
      "C. Lawrence Zitnick. Microsoft COCO: Common objects\n",
      "in context. In European Conference on Computer Vision\n",
      "(ECCV) , volume 8693, pages 740–755, 2014. 5, 6, 11\n",
      "[56] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\n",
      "ard Pons-Moll, and Michael J. Black. SMPL: A skinned\n",
      "multi-person linear model. Transactions on Graphics (TOG) ,\n",
      "34(6):248:1–248:16, 2015. 2, 3\n",
      "[57] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-\n",
      "Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-\n",
      "Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh\n",
      "Chang, Wei Hua, Manfred Georg, and Matthias Grundmann.\n",
      "MediaPipe: A framework for building perception pipelines.\n",
      "InComputer Vision and Pattern Recognition Workshops\n",
      "(CVPRw) , 2019. 11\n",
      "[58] Yiyue Luo, Yunzhu Li, Michael Foshey, Wan Shou,\n",
      "Pratyusha Sharma, Tom ´as Palacios, Antonio Torralba, and\n",
      "Wojciech Matusik. Intelligent carpet: Inferring 3D human\n",
      "pose from tactile signals. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 11255–11265, 2021. 3\n",
      "[59] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\n",
      "ard Pons-Moll, and Michael J. Black. AMASS: Archive of\n",
      "motion capture as surface shapes. In International Confer-\n",
      "16\n",
      "ence on Computer Vision (ICCV) , pages 5441–5450, 2019.\n",
      "9\n",
      "[60] Khaled Mamou, E Lengyel, and A Peters. V olumetric hierar-\n",
      "chical approximate convex decomposition. In Game Engine\n",
      "Gems 3 , pages 141–158. AK Peters, 2016. 12\n",
      "[61] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal V .\n",
      "Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\n",
      "Theobalt. Monocular 3D human pose estimation in the wild\n",
      "using improved CNN supervision. International Conference\n",
      "on 3D Vision (3DV) , pages 506–516, 2017. 3, 5, 6, 9, 11\n",
      "[62] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,\n",
      "Helge Rhodin, Mohammad Shafiei, Hans-Peter Seidel,\n",
      "Weipeng Xu, Dan Casas, and Christian Theobalt. VNect:\n",
      "Real-time 3D human pose estimation with a single RGB\n",
      "camera. Transactions on Graphics (TOG) , 36(4):44:1–44:14,\n",
      "2017. 3\n",
      "[63] Gyeongsik Moon and Kyoung Mu Lee. I2L-MeshNet:\n",
      "Image-to-lixel prediction network for accurate 3D human\n",
      "pose and mesh estimation from a single RGB image. In\n",
      "European Conference on Computer Vision (ECCV) , volume\n",
      "12352, pages 752–768, 2020. 3\n",
      "[64] Lea M ¨uller, Ahmed A. A. Osman, Siyu Tang, Chun-Hao P.\n",
      "Huang, and Michael J. Black. On self-contact and human\n",
      "pose. In Computer Vision and Pattern Recognition (CVPR) ,\n",
      "pages 9990–9999, 2021. 1, 2, 3, 4, 5, 6, 8, 11, 12\n",
      "[65] NVIDIA PhysX: A scalable multi-platform physics simu-\n",
      "lation solution. https://developer.nvidia.com/\n",
      "physx-sdk . 1\n",
      "[66] Yi-Chung Pai. Movement termination and stability in stand-\n",
      "ing.Exercise and sport sciences reviews , 31(1):19–25, 2003.\n",
      "2, 4\n",
      "[67] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch,\n",
      "David T Hoffmann, Shashank Tripathi, and Michael J Black.\n",
      "AGORA: Avatars in geography optimized for regression\n",
      "analysis. In Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 13468–13478, 2021. 6, 9\n",
      "[68] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\n",
      "Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas,\n",
      "and Michael J. Black. Expressive body capture: 3D hands,\n",
      "face, and body from a single image. In Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 10975–10985, 2019. 2,\n",
      "3, 6, 9, 12, 13\n",
      "[69] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel\n",
      "van de Panne. DeepMimic: Example-guided deep reinforce-\n",
      "ment learning of physics-based character skills. Transactions\n",
      "on Graphics (TOG) , 37(4):1–14, 2018. 2, 3\n",
      "[70] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\n",
      "Srinath Sridhar, and Leonidas J. Guibas. HuMoR: 3D hu-\n",
      "man motion model for robust pose estimation. In Inter-\n",
      "national Conference on Computer Vision (ICCV) , pages\n",
      "11468–11479, 2021. 3\n",
      "[71] Davis Rempe, Leonidas J. Guibas, Aaron Hertzmann, Bryan\n",
      "Russell, Ruben Villegas, and Jimei Yang. Contact and hu-\n",
      "man dynamics from monocular video. In European Con-\n",
      "ference on Computer Vision (ECCV) , volume 12350, pages\n",
      "71–87, 2020. 1, 3\n",
      "[72] Ralph Tyrell Rockafellar. Convex analysis . Princeton uni-\n",
      "versity press, 2015. 4[73] Gr´egory Rogez, James Steven Supancic, and Deva Ramanan.\n",
      "Understanding everyday hands in action from RGB-D im-\n",
      "ages. In International Conference on Computer Vision\n",
      "(ICCV) , pages 3889–3897, 2015. 2, 4\n",
      "[74] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. FrankMocap:\n",
      "A monocular 3D whole-body pose estimation system via\n",
      "regression and integration. In International Conference on\n",
      "Computer Vision Workshops (ICCVw) , pages 1749–1759,\n",
      "2021. 3\n",
      "[75] Nadine Rueegg, Shashank Tripathi, Konrad Schindler,\n",
      "Michael J. Black, and Silvia Zuffi. BITE: Beyond priors for\n",
      "improved three-D dog pose estimation. In Computer Vision\n",
      "and Pattern Recognition (CVPR) , June 2023. 3\n",
      "[76] Jesse Scott, Bharadwaj Ravichandran, Christopher Funk,\n",
      "Robert T Collins, and Yanxi Liu. From image to stability:\n",
      "Learning dynamics from human pose. In European Con-\n",
      "ference on Computer Vision (ECCV) , volume 12368, pages\n",
      "536–554, 2020. 2, 3, 4\n",
      "[77] Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Ko-\n",
      "mura, Dani Lischinski, Daniel Cohen-Or, and Baoquan\n",
      "Chen. MotioNet: 3D human motion reconstruction from\n",
      "monocular video with skeleton consistency. Transactions\n",
      "on Graphics (TOG) , 40(1):1:1–1:15, 2021. 3\n",
      "[78] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick\n",
      "P´erez, and Christian Theobalt. Neural monocular 3D human\n",
      "motion capture with physical awareness. Transactions on\n",
      "Graphics (TOG) , 40(4), 2021. 3\n",
      "[79] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Chris-\n",
      "tian Theobalt. PhysCap: Physically plausible monocular\n",
      "3D motion capture in real time. Transactions on Graphics\n",
      "(TOG) , 39(6):235:1–235:16, 2020. 1, 2, 3, 8\n",
      "[80] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J. Black, and\n",
      "Tao Mei. Monocular, one-stage, regression of multiple 3D\n",
      "people. In International Conference on Computer Vision\n",
      "(ICCV) , pages 11179–11188, 2021. 1, 3\n",
      "[81] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao\n",
      "Mei. Human mesh recovery from monocular images via\n",
      "a skeleton-disentangled representation. In International\n",
      "Conference on Computer Vision (ICCV) , pages 5348–5357,\n",
      "2019. 3\n",
      "[82] Yating Tian, Hongwen Zhang, Yebin Liu, and limin Wang.\n",
      "Recovering 3D human mesh from monocular images: A\n",
      "survey. arXiv:2203.01923 , 2022. 3\n",
      "[83] Shashank Tripathi, Siddhant Ranade, Ambrish Tyagi, and\n",
      "Amit K. Agrawal. PoseNet3D: Learning temporally con-\n",
      "sistent 3D human pose via knowledge distillation. In Inter-\n",
      "national Conference on 3D Vision (3DV) , pages 311–321,\n",
      "2020. 3\n",
      "[84] Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo\n",
      "Aponte, Marc Pollefeys, and Juergen Gall. Capturing hands\n",
      "in action using discriminative salient points and physics sim-\n",
      "ulation. International Journal of Computer Vision (IJCV) ,\n",
      "118:172–193, 2016. 6, 12\n",
      "[85] Laurens van der Maaten and Geoffrey Hinton. Visualizing\n",
      "data using t-sne. Journal of Machine Learning Research\n",
      "(JMLR) , 9(86):2579–2605, 2008. 9\n",
      "17\n",
      "[86] Manisha Verma, Sudhakar Kumawat, Yuta Nakashima, and\n",
      "Shanmuganathan Raman. Yoga-82: A new dataset for fine-\n",
      "grained classification of human poses. In Computer Vision\n",
      "and Pattern Recognition Workshops (CVPRw) , pages 1038–\n",
      "1039, 2020. 9\n",
      "[87] Ruben Villegas, Duygu Ceylan, Aaron Hertzmann, Jimei\n",
      "Yang, and Jun Saito. Contact-aware retargeting of skinned\n",
      "motion. In International Conference on Computer Vision\n",
      "(ICCV) , pages 9720–9729, 2021. 3, 10\n",
      "[88] Timo von Marcard, Roberto Henschel, Michael J. Black,\n",
      "Bodo Rosenhahn, and Gerard Pons-Moll. Recovering ac-\n",
      "curate 3D human pose in the wild using IMUs and a mov-\n",
      "ing camera. In European Conference on Computer Vision\n",
      "(ECCV) , volume 11214, pages 614–631, 2018. 5, 12, 13\n",
      "[89] Marek V ondrak, Leonid Sigal, and Odest Chadwicke Jenkins.\n",
      "Physical simulation for probabilistic motion tracking. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "1–8, 2008. 3\n",
      "[90] Eric W. Weisstein. Triangle point picking.\n",
      "https : / / mathworld . wolfram . com /\n",
      "TrianglePointPicking . html , 2014. From\n",
      "MathWorld – A Wolfram Web Resource. 4\n",
      "[91] Zhenzhen Weng and Serena Yeung. Holistic 3D human\n",
      "and scene mesh estimation from single view images. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "334–343, 2020. 3\n",
      "[92] David A. Winter. A.B.C. (Anatomy, Biomechanics and Con-\n",
      "trol) of balance during standing and walking . Waterloo\n",
      "Biomechanics, 1995. 2, 4, 5\n",
      "[93] David A. Winter. Human balance and posture control during\n",
      "standing and walking. Gait & Posture , 3(4):193–214, 1995.\n",
      "2, 5\n",
      "[94] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular\n",
      "total capture: Posing face, body, and hands in the wild. In\n",
      "Computer Vision and Pattern Recognition (CVPR) , pages\n",
      "10957–10966, 2019. 3\n",
      "[95] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica\n",
      "Hodgins. MonoClothCap: Towards temporally coherent\n",
      "clothing capture from monocular RGB video. In Interna-\n",
      "tional Conference on 3D Vision (3DV) , pages 322–332, 2020.\n",
      "3\n",
      "[96] Kevin Xie, Tingwu Wang, Umar Iqbal, Yunrong Guo, Sanja\n",
      "Fidler, and Florian Shkurti. Physics-based human motion\n",
      "estimation and synthesis from videos. In International Con-\n",
      "ference on Computer Vision (ICCV) , pages 11532–11541,\n",
      "2021. 3, 8\n",
      "[97] Xianghui Xie, Bharat Lal Bhatnagar, and Gerard Pons-Moll.\n",
      "CHORE: Contact, human and object reconstruction from a\n",
      "single RGB image. In European Conference on Computer\n",
      "Vision (ECCV) , 2022. 3\n",
      "[98] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas,\n",
      "and Michael J. Black. ECON: Explicit clothed humans\n",
      "optimized via normal integration. In Computer Vision and\n",
      "Pattern Recognition (CVPR) , 2023. 3\n",
      "[99] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,\n",
      "William T. Freeman, Rahul Sukthankar, and Cristian Smin-\n",
      "chisescu. GHUM & GHUML: Generative 3D human shapeand articulated pose models. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 6183–6192, 2020. 2, 3\n",
      "[100] Masanobu Yamamoto and Katsutoshi Yagishita. Scene\n",
      "constraints-aided tracking of human body. In Computer\n",
      "Vision and Pattern Recognition (CVPR) , pages 151–156,\n",
      "2000. 3\n",
      "[101] Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea\n",
      "Hering, Justus Thies, and Michael J. Black. MIME: Human-\n",
      "aware 3D scene generation. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , June 2023. 3\n",
      "[102] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and\n",
      "Jan Kautz. GLAMR: Global occlusion-aware human mesh\n",
      "recovery with dynamic cameras. In Computer Vision and\n",
      "Pattern Recognition (CVPR) , pages 11028–11039, 2022. 12\n",
      "[103] Ye Yuan and Kris Kitani. 3D ego-pose estimation via imita-\n",
      "tion learning. In European Conference on Computer Vision\n",
      "(ECCV) , volume 11220, pages 735–750, 2018. 2\n",
      "[104] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and\n",
      "Jason Saragih. SimPoE: Simulated character control for 3D\n",
      "human pose estimation. In Computer Vision and Pattern\n",
      "Recognition (CVPR) , pages 7159–7169, 2021. 1, 2, 3, 6, 8\n",
      "[105] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu,\n",
      "William T Freeman, Rahul Sukthankar, and Cristian Smin-\n",
      "chisescu. Weakly supervised 3D human pose and shape\n",
      "reconstruction with normalizing flows. In European Con-\n",
      "ference on Computer Vision (ECCV) , pages 465–481, 2020.\n",
      "3\n",
      "[106] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-\n",
      "escu. Monocular 3D pose and shape estimation of multiple\n",
      "people in natural scenes – the importance of multiple scene\n",
      "constraints. In Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 2148–2157, 2018. 3, 6, 8, 10\n",
      "[107] Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang,\n",
      "and Qiang Xu. SmoothNet: A plug-and-play network for\n",
      "refining human poses in videos. In European Conference on\n",
      "Computer Vision (ECCV) , volume 13665, pages 625–642,\n",
      "2022. 3\n",
      "[108] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and\n",
      "Xiaogang Wang. 3D human mesh regression with dense cor-\n",
      "respondence. In Computer Vision and Pattern Recognition\n",
      "(CVPR) , 2020. 3\n",
      "[109] Cha Zhang and Tsuhan Chen. Efficient feature extraction\n",
      "for 2d/3d objects in mesh representation. In Proceedings\n",
      "2001 International Conference on Image Processing (Cat.\n",
      "No. 01CH37205) , volume 3, pages 935–938. IEEE, 2001. 4\n",
      "[110] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\n",
      "Yebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D hu-\n",
      "man pose and shape regression with pyramidal mesh align-\n",
      "ment feedback loop. In International Conference on Com-\n",
      "puter Vision (ICCV) , pages 11426–11436, 2021. 1, 3\n",
      "[111] Jianfeng Zhang, Dongdong Yu, Jun Hao Liew, Xuecheng\n",
      "Nie, and Jiashi Feng. Body meshes as points. In Computer\n",
      "Vision and Pattern Recognition (CVPR) , pages 546–556,\n",
      "2021. 3\n",
      "[112] Jason Y . Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan,\n",
      "Jitendra Malik, and Angjoo Kanazawa. Perceiving 3D\n",
      "human-object spatial arrangements from a single image in\n",
      "18\n",
      "the wild. In European Conference on Computer Vision\n",
      "(ECCV) , volume 12357, pages 34–51, 2020. 3, 10\n",
      "[113] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys,\n",
      "and Siyu Tang. Learning motion priors for 4D human body\n",
      "capture in 3D scenes. In International Conference on Com-\n",
      "puter Vision (ICCV) , pages 11343–11353, 2021. 3, 10\n",
      "[114] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-\n",
      "occluded human shape and pose estimation from a single\n",
      "color image. In Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 7374–7383, 2020. 3\n",
      "[115] Ce Zheng, Wenhan Wu, Chen Chen, Taojiannan Yang, Si-\n",
      "jie Zhu, Ju Shen, Nasser Kehtarnavaz, and Mubarak Shah.\n",
      "Deep learning-based human pose estimation: A survey.\n",
      "arXiv:2012.13392 , 2022. 3[116] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\n",
      "Li. On the continuity of rotation representations in neural\n",
      "networks. In Computer Vision and Pattern Recognition\n",
      "(CVPR) , pages 5745–5753, 2019. 3\n",
      "[117] Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, Ayush\n",
      "Tewari, Christian Theobalt, and Feng Xu. Monocular real-\n",
      "time full body capture with inter-part correlations. In Com-\n",
      "puter Vision and Pattern Recognition (CVPR) , pages 4811–\n",
      "4822, 2021. 3\n",
      "[118] Yuliang Zou, Jimei Yang, Duygu Ceylan, Jianming Zhang,\n",
      "Federico Perazzi, and Jia-Bin Huang. Reducing footskate\n",
      "in human motion reconstruction with ground contact con-\n",
      "straints. In Winter Conference on Applications of Computer\n",
      "Vision (WACV) , pages 459–468, 2020. 3, 6, 8\n",
      "19\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le début de l'article est comme suit :\n",
    "\n",
    "\n",
    "![titre.png](data:image/png;base64,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)\n",
    "\n",
    "L'extraction est très différente :\n",
    "\n",
    "3D Human Pose Estimation via Intuitive Physics\n",
    "\n",
    "Shashank Tripathi1Lea M ¨uller1Chun-Hao P. Huang1Omid Taheri1\n",
    "\n",
    "Michael J. Black1Dimitrios Tzionas2*\n",
    "\n",
    "1Max Planck Institute for Intelligent Systems, T ¨ubingen, \n",
    "\n",
    "Germany2University of Amsterdam, the Netherlands\n",
    "\n",
    "{stripathi, lmueller2, chuang2, otaheri, black }@tue.mpg.de d.tzionas@uva.nl\n",
    "\n",
    "La récupération des affiliations est donc une tache complexe (outre les problèmes d'accentuation...)."
   ],
   "metadata": {
    "id": "HAz_fIRYrwuM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sur les références (bibliographie), l'extraction est très propre avec seulement quelques numéro de pages qui apparaissent.\n",
    "On observe des liens vers le corps du document (ce qui est inabituel)."
   ],
   "metadata": {
    "id": "BzaCoUr4wX6H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Métadonnées des fichiers pdf\n",
    "\n",
    "Regardons les métadonnées des deux fichiers PDF téléchargés."
   ],
   "metadata": {
    "id": "8XZoXjM1ydT-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def printMetadata(file):\n",
    "  with open(file, \"rb\") as pdf_file:\n",
    "    read_pdf = PyPDF2.PdfReader(pdf_file)\n",
    "    for k,v in read_pdf.metadata.items():\n",
    "      print(k,v)"
   ],
   "metadata": {
    "id": "YklgXupPZilT"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "printMetadata(filename)\n",
    "print('\\n')\n",
    "printMetadata(filename2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rG630VMxYYye",
    "outputId": "483d85d3-928c-4274-bf2c-13807f50b5ee"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Producer dvips + GPL Ghostscript GIT PRERELEASE 9.22\n",
      "/CreationDate D:20180910172622-04'00'\n",
      "/ModDate D:20180910172622-04'00'\n",
      "/Creator LaTeX with hyperref package\n",
      "/Title \n",
      "/Subject \n",
      "/Author \n",
      "/Keywords \n",
      "\n",
      "\n",
      "/Author \n",
      "/CreationDate D:20230407004730Z\n",
      "/Creator LaTeX with hyperref\n",
      "/Keywords \n",
      "/ModDate D:20230407004730Z\n",
      "/PTEX.Fullbanner This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2\n",
      "/Producer pdfTeX-1.40.21\n",
      "/Subject \n",
      "/Title \n",
      "/Trapped /False\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Malheureusement, les métadonnées sont globalement inutilisées. Ce n'est donc pas un moyen fiable pour récuperer les mots-clés."
   ],
   "metadata": {
    "id": "rULWTr8uyjYa"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
